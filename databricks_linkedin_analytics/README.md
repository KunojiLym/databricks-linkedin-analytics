# Build Your Own LinkedIn Analytics (2025): Reference Architecture for Databricks Free Edition

The 'databricks_linkedin_analytics' project was generated by using the default template.

* `src/`: Python source code for this project.
* `resources/`:  Resource configurations (jobs, pipelines, etc.)

## Getting started

Choose how you want to work on this project:

(a) Directly in your Databricks workspace, see
    https://docs.databricks.com/dev-tools/bundles/workspace.

(b) Locally with an IDE like Cursor or VS Code, see
    https://docs.databricks.com/dev-tools/vscode-ext.html.

(c) With command line tools, see https://docs.databricks.com/dev-tools/cli/databricks-cli.html

## Using this project using the CLI

The Databricks workspace and IDE extensions provide a graphical interface for working
with this project. It's also possible to interact with it directly using the CLI:

1. Authenticate to your Databricks workspace, if you have not done so already:
    ```
    $ databricks configure
    ```

2. To deploy a development copy of this project, type:
    ```
    $ databricks bundle deploy --target dev
    ```
    (Note that "dev" is the default target, so the `--target` parameter
    is optional here.)

    This deploys everything that's defined for this project.
    For example, the default template would deploy a pipeline called
    `[dev yourname] databricks_linkedin_analytics_etl` to your workspace.
    You can find that resource by opening your workpace and clicking on **Jobs & Pipelines**.

3. Similarly, to deploy a production copy, type:
   ```
   $ databricks bundle deploy --target prod
   ```
   Note the default template has a includes a job that runs the pipeline every day
   (defined in resources/sample_job.job.yml). The schedule
   is paused when deploying in development mode (see
   https://docs.databricks.com/dev-tools/bundles/deployment-modes.html).

4. To run a job or pipeline, use the "run" command:
   ```
   $ databricks bundle run
   ```

## Documentation

This repository's documentation is intentionally concise and organized for quick navigation without needing the original blog posts. The docs are high-level—implementation details live in the code (notebooks, SQL, and YAML) so that documentation stays stable and low-effort to maintain.

### Primary entry point ###
- `docs/README.md` — consolidated documentation index with short, one-line descriptions and direct links to detailed pages.

### Quick links ###
- Quickstart: `docs/quickstart.md` — deploy to Databricks and run notebooks
- Architecture: `docs/architecture.md` — high-level architecture and component mapping
- Ingestion: `docs/ingestion.md` — bronze ingestion patterns and where to find notebooks
- Transformation: `docs/transformation.md` — silver transformations overview
- Modeling: `docs/modeling.md` — gold modeling and SQL DDL
- Dashboards: `docs/dashboard_design.md` — dashboard design and the data product
- Orchestration: `docs/orchestration.md` — jobs and pipelines wiring
- Observability: `docs/observability.md` — monitoring and run checks
- Maintainability: `docs/maintainability.md` — guidelines for code organization and changes
- Key takeaways: `docs/TODO.md` — actionable TODO list and next steps

### Repository as source-of-truth ###
- For low-friction maintenance, treat the code as the authoritative source. If you need details about transformation logic, open the corresponding notebook or SQL in `src/`.
- Configuration and deploy-time variables live in `databricks_linkedin_analytics/resources/variables.yml` and `resources/schemas.yml`.

### Changelog ###
- See the top-level `CHANGELOG.md` for a running history of repository changes and releases.

See `CONTRIBUTING.md` for how to propose changes and the PR checklist.
