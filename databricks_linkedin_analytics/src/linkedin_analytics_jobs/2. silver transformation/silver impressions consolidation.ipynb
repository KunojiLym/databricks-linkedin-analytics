{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "419bf894-f562-4702-ba83-9e7afa6d2633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Silver Layer: Impressions ETL\n",
    "\n",
    "This section processes daily bronze impressions tables, standardizes date formats, merges data into the silver impressions table, and logs ingestion events. Source tables are dropped after processing to maintain workspace hygiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d399bdd-f3fc-4b3f-b311-0649a09da8a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import sys\n",
    "\n",
    "    sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../utils')))\n",
    "\n",
    "    from pipeline_utils import get_valid_parameter\n",
    "except:\n",
    "    import re\n",
    "\n",
    "    def get_valid_parameter(parameter_key: str):\n",
    "        \"\"\"\n",
    "        Get a valid parameter value from the Databricks widgets.\n",
    "        Hardened to thwart SQL injection attacks.\n",
    "        \"\"\"\n",
    "        parameter_value = dbutils.widgets.get(parameter_key)\n",
    "        \n",
    "        # Parameter_value must be a string with only alphanumeric characters and underscores\n",
    "        if not re.fullmatch(r'[a-zA-Z0-9_]+', parameter_value):\n",
    "            raise ValueError(f\"Invalid parameter value for {parameter_key}: {parameter_value}\")\n",
    "        \n",
    "        # Disallow dangerous SQL keywords and patterns\n",
    "        forbidden_patterns = [\n",
    "            r'--', r';', r\"'\", r'\"', r'/\\*', r'\\*/', r'xp_', r'char\\(', r'nchar\\(', r'varchar\\(', r'\\balter\\b', r'\\bdrop\\b', r'\\binsert\\b', r'\\bdelete\\b', r'\\bupdate\\b', r'\\bselect\\b', r'\\bcreate\\b', r'\\bexec\\b', r'\\bunion\\b', r'\\bor\\b', r'\\band\\b'\n",
    "        ]\n",
    "        for pattern in forbidden_patterns:\n",
    "            if re.search(pattern, parameter_value, re.IGNORECASE):\n",
    "                raise ValueError(f\"Potentially dangerous value for {parameter_key}: {parameter_value} (pattern matched: {pattern})\")\n",
    "        return parameter_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "642c4674-160b-49f2-b9d7-2944a03dddf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import sum\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# 1. define our constants\n",
    "try:\n",
    "    BRONZE_CATALOG = get_valid_parameter(\"BRONZE_CATALOG\")\n",
    "    BRONZE_SCHEMA = get_valid_parameter(\"BRONZE_SCHEMA\")\n",
    "    BRONZE_IMPRESSIONS_TABLE_PREFIX = get_valid_parameter(\"BRONZE_IMPRESSIONS_TABLE_PREFIX\")\n",
    "    BRONZE_TOTALS_TABLE = get_valid_parameter(\"BRONZE_TOTALS_TABLE\")\n",
    "\n",
    "    SILVER_CATALOG = get_valid_parameter(\"SILVER_CATALOG\")\n",
    "    SILVER_SCHEMA = get_valid_parameter(\"SILVER_SCHEMA\")\n",
    "    SILVER_IMPRESSIONS_TABLE = get_valid_parameter(\"SILVER_IMPRESSIONS_TABLE\")\n",
    "\n",
    "    print(\"Loaded all widget values\")\n",
    "except:\n",
    "    BRONZE_CATALOG = \"bronze\"\n",
    "    BRONZE_SCHEMA = \"linkedin\"\n",
    "    BRONZE_IMPRESSIONS_TABLE_PREFIX = \"impressions\"\n",
    "    BRONZE_TOTALS_TABLE = \"totals\"\n",
    "\n",
    "    SILVER_CATALOG = \"silver\"\n",
    "    SILVER_SCHEMA = \"linkedin\"\n",
    "    SILVER_IMPRESSIONS_TABLE = \"impressions\"\n",
    "\n",
    "    print(\"Failed to load widget values, using default values\")\n",
    "\n",
    "# 2. set our input and output variables\n",
    "\n",
    "# extract list of daily bronze staging impressions tables\n",
    "bronze_impressions_tables = [\n",
    "    table.name for table in spark.catalog.listTables(f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}\")\n",
    "    if re.match(rf\"{BRONZE_IMPRESSIONS_TABLE_PREFIX}_\\d{{4}}_\\d{{2}}_\\d{{2}}$\", table.name)\n",
    "]\n",
    "\n",
    "silver_impressions_table = \\\n",
    "  f\"{SILVER_CATALOG}.{SILVER_SCHEMA}.{SILVER_IMPRESSIONS_TABLE}\"\n",
    "\n",
    "# 3. execute the ingestion\n",
    "ingestion_timestamp = datetime.datetime.utcnow()\n",
    "\n",
    "# Process daily bronze staging impressions tables and merge into silver layer\n",
    "\n",
    "print(\"Processing daily impressions tables...\")\n",
    "\n",
    "for bronze_impression_table in bronze_impressions_tables:\n",
    "    # Extract date suffix from table name and convert to datetime in %Y_%m_%d format\n",
    "    date_str = bronze_impression_table.replace(f\"{BRONZE_IMPRESSIONS_TABLE_PREFIX}_\", \"\")\n",
    "    analytics_date = pd.to_datetime(date_str, format='%Y_%m_%d').date()\n",
    "\n",
    "    table_name = f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{bronze_impression_table}\"\n",
    "    \n",
    "    print(f\"Processing {table_name}...\")\n",
    "\n",
    "    silver_impressions_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        post_url,\n",
    "        to_date(post_publish_date, 'M/d/yyyy') AS post_publish_date,\n",
    "        impressions,\n",
    "        to_date('{date_str}', 'yyyy_MM_dd') AS analytics_date \n",
    "    FROM {table_name}\n",
    "    \"\"\")\n",
    "\n",
    "    silver_impressions_totals_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        impressions,\n",
    "        source_file,\n",
    "        source_file_timestamp,\n",
    "        ingestion_timestamp\n",
    "    FROM {BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_TOTALS_TABLE}\n",
    "    WHERE to_date('{date_str}', 'yyyy_MM_dd') = date\n",
    "    \"\"\")\n",
    "\n",
    "    impressions_others = silver_impressions_totals_df.select(\"impressions\").collect()[0].impressions - silver_impressions_df.agg(sum(\"impressions\")).collect()[0][0]\n",
    "    if impressions_others > 0:\n",
    "        silver_impressions_df = silver_impressions_df.union(\n",
    "            spark.createDataFrame([\n",
    "                Row(\n",
    "                    post_url = \"others\",\n",
    "                    post_publish_date = analytics_date,\n",
    "                    impressions = impressions_others,\n",
    "                    analytics_date = analytics_date,\n",
    "                    # ingestion_timestamp = silver_impressions_totals_df.select(\"ingestion_timestamp\").collect()[0].ingestion_timestamp,\n",
    "                    # source_file = silver_impressions_totals_df.select(\"source_file\").collect()[0].source_file,\n",
    "                    # source_file_timestamp = silver_impressions_totals_df.select(\"source_file_timestamp\").collect()[0].source_file_timestamp\n",
    "                )\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    if spark.catalog.tableExists(silver_impressions_table):\n",
    "        print(f\"Table {silver_impressions_table} exists, merging data...\")\n",
    "        delta_table = DeltaTable.forName(spark, silver_impressions_table)\n",
    "        (\n",
    "            delta_table.alias(\"t\")\n",
    "            .merge(\n",
    "                silver_impressions_df.alias(\"s\"),\n",
    "                \"t.post_url = s.post_url AND t.analytics_date = s.analytics_date\"\n",
    "            )\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Table {silver_impressions_table} does not exist, creating...\")\n",
    "        silver_impressions_df.write.format(\"delta\").saveAsTable(silver_impressions_table)\n",
    "    \n",
    "    # Drop the source table after processing\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    print(f\"Dropped table {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5123ec9d-bebd-48ed-b185-7f1a41d37329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Silver Layer: Fill in-between dates for impressions tables with 0 values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a14304ad-9d23-425a-9478-31e6672f43b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, sequence, explode, to_date, min as spark_min, max as spark_max\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import sum\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# 1. define our constants\n",
    "try:\n",
    "    SILVER_CATALOG = get_valid_parameter(\"SILVER_CATALOG\")\n",
    "    SILVER_SCHEMA = get_valid_parameter(\"SILVER_SCHEMA\")\n",
    "    SILVER_IMPRESSIONS_TABLE = get_valid_parameter(\"SILVER_IMPRESSIONS_TABLE\")\n",
    "\n",
    "    print(\"Loaded all widget values\")\n",
    "except:\n",
    "    SILVER_CATALOG = \"silver\"\n",
    "    SILVER_SCHEMA = \"linkedin\"\n",
    "    SILVER_IMPRESSIONS_TABLE = \"impressions\"\n",
    "\n",
    "    print(\"Failed to load widget values, using default values\")\n",
    "\n",
    "silver_impressions_table = \\\n",
    "  f\"{SILVER_CATALOG}.{SILVER_SCHEMA}.{SILVER_IMPRESSIONS_TABLE}\"\n",
    "\n",
    "\n",
    "try:\n",
    "    import sys\n",
    "\n",
    "    sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../utils')))\n",
    "\n",
    "    from pipeline_utils import fill_missing_dates\n",
    "except:\n",
    "    def fill_missing_dates(df, date_col, group_cols, value_col):\n",
    "        # Get min and max dates\n",
    "        date_range = df.select(spark_min(date_col).alias(\"min_date\"), spark_max(date_col).alias(\"max_date\")).collect()[0]\n",
    "        min_date, max_date = date_range.min_date, date_range.max_date\n",
    "\n",
    "        # Create full date sequence\n",
    "        date_seq_df = spark.createDataFrame([(min_date, max_date)], [\"start\", \"end\"]) \\\n",
    "            .select(explode(sequence(col(\"start\"), col(\"end\"))).alias(date_col))\n",
    "\n",
    "        # Cross join with unique post_url and post_publish_date\n",
    "        unique_keys_df = df.select(*group_cols).distinct()\n",
    "        full_grid_df = unique_keys_df.crossJoin(date_seq_df)\n",
    "\n",
    "        # Left join to original df\n",
    "        filled_df = full_grid_df.join(\n",
    "            df,\n",
    "            on=group_cols + [date_col],\n",
    "            how=\"left\"\n",
    "        ).fillna({value_col: 0})\n",
    "\n",
    "        return filled_df\n",
    "\n",
    "# Fill missing dates for silver impressions\n",
    "silver_impressions_filled_df = fill_missing_dates(\n",
    "    spark.table(silver_impressions_table),\n",
    "    \"analytics_date\",\n",
    "    [\"post_url\", \"post_publish_date\"],\n",
    "    \"impressions\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6840877729571798,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver impressions consolidation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
