{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f934943-fdfe-4d70-9533-ce9c6246b4ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Silver Layer: Engagements ETL\n",
    "This section processes daily bronze engagements tables, standardizes date formats, merges data into the silver engagemetns table, and logs ingestion events. Source tables are dropped after processing to maintain workspace hygiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b418b66d-3168-46b9-bff0-31e32eae44f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_valid_parameter(parameter_key: str):\n",
    "    \"\"\"\n",
    "    Get a valid parameter value from the Databricks widgets.\n",
    "    Hardened to thwart SQL injection attacks.\n",
    "    \"\"\"\n",
    "    parameter_value = dbutils.widgets.get(parameter_key)\n",
    "    \n",
    "    # Parameter_value must be a string with only alphanumeric characters and underscores\n",
    "    if not re.fullmatch(r'[a-zA-Z0-9_]+', parameter_value):\n",
    "        raise ValueError(f\"Invalid parameter value for {parameter_key}: {parameter_value}\")\n",
    "    \n",
    "    # Disallow dangerous SQL keywords and patterns\n",
    "    forbidden_patterns = [\n",
    "        r'--', r';', r\"'\", r'\"', r'/\\*', r'\\*/', r'xp_', r'char\\(', r'nchar\\(', r'varchar\\(', r'\\balter\\b', r'\\bdrop\\b', r'\\binsert\\b', r'\\bdelete\\b', r'\\bupdate\\b', r'\\bselect\\b', r'\\bcreate\\b', r'\\bexec\\b', r'\\bunion\\b', r'\\bor\\b', r'\\band\\b'\n",
    "    ]\n",
    "    for pattern in forbidden_patterns:\n",
    "        if re.search(pattern, parameter_value, re.IGNORECASE):\n",
    "            raise ValueError(f\"Potentially dangerous value for {parameter_key}: {parameter_value} (pattern matched: {pattern})\")\n",
    "    return parameter_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b0b3e1-f407-487c-896a-144e49812cae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import sum\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# 1. define our constants\n",
    "try:\n",
    "    BRONZE_CATALOG = get_valid_parameter(\"BRONZE_CATALOG\")\n",
    "    BRONZE_SCHEMA = get_valid_parameter(\"BRONZE_SCHEMA\")\n",
    "    BRONZE_ENGAGEMENTS_TABLE_PREFIX = get_valid_parameter(\"BRONZE_ENGAGEMENTS_TABLE_PREFIX\")\n",
    "    BRONZE_TOTALS_TABLE = get_valid_parameter(\"BRONZE_TOTALS_TABLE\")\n",
    "\n",
    "    SILVER_CATALOG = get_valid_parameter(\"SILVER_CATALOG\")\n",
    "    SILVER_SCHEMA = get_valid_parameter(\"SILVER_SCHEMA\")\n",
    "    SILVER_ENGAGEMENTS_TABLE = get_valid_parameter(\"SILVER_ENGAGEMENTS_TABLE\")\n",
    "\n",
    "    print(\"Loaded all widget values\")\n",
    "except:\n",
    "    BRONZE_CATALOG = \"bronze\"\n",
    "    BRONZE_SCHEMA = \"linkedin\"\n",
    "    BRONZE_ENGAGEMENTS_TABLE_PREFIX = \"engagements\"\n",
    "    BRONZE_TOTALS_TABLE = \"totals\"\n",
    "\n",
    "    SILVER_CATALOG = \"silver\"\n",
    "    SILVER_SCHEMA = \"linkedin\"\n",
    "    SILVER_ENGAGEMENTS_TABLE = \"engagements\"\n",
    "\n",
    "    print(\"Failed to load widget values, using default values\")\n",
    "\n",
    "# 2. set our input and output variables\n",
    "\n",
    "# extract list of daily bronze staging engagements tables\n",
    "bronze_engagements_tables = [\n",
    "    table.name for table in spark.catalog.listTables(f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}\")\n",
    "    if re.match(rf\"{BRONZE_ENGAGEMENTS_TABLE_PREFIX}_\\d{{4}}_\\d{{2}}_\\d{{2}}$\", table.name)\n",
    "]\n",
    "\n",
    "silver_engagements_table = \\\n",
    "  f\"{SILVER_CATALOG}.{SILVER_SCHEMA}.{SILVER_ENGAGEMENTS_TABLE}\"\n",
    "\n",
    "# 3. execute the ingestion\n",
    "ingestion_timestamp = datetime.datetime.utcnow()\n",
    "\n",
    "# Process daily bronze staging engagements tables and merge into silver layer\n",
    "\n",
    "print(\"Processing daily engagements tables...\")\n",
    "\n",
    "for bronze_engagements_table in bronze_engagements_tables:\n",
    "    # Extract date suffix from table name and convert to datetime in %Y_%m_%d format\n",
    "    date_str = bronze_engagements_table.replace(f\"{BRONZE_ENGAGEMENTS_TABLE_PREFIX}_\", \"\")\n",
    "    analytics_date = pd.to_datetime(date_str, format='%Y_%m_%d').date()\n",
    "\n",
    "    table_name = f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{bronze_engagements_table}\"\n",
    "    \n",
    "    print(f\"Processing {table_name}...\")\n",
    "\n",
    "    silver_engagements_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        post_url,\n",
    "        to_date(post_publish_date, 'M/d/yyyy') AS post_publish_date,\n",
    "        engagements,\n",
    "        to_date('{date_str}', 'yyyy_MM_dd') AS analytics_date \n",
    "    FROM {table_name}\n",
    "    \"\"\")\n",
    "\n",
    "    silver_engagements_totals_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        engagements,\n",
    "        source_file,\n",
    "        source_file_timestamp,\n",
    "        ingestion_timestamp\n",
    "    FROM {BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_TOTALS_TABLE}\n",
    "    WHERE to_date('{date_str}', 'yyyy_MM_dd') = date\n",
    "    \"\"\")\n",
    "\n",
    "    engagements_others = silver_engagements_totals_df.select(\"engagements\").collect()[0].engagements - silver_engagements_df.agg(sum(\"engagements\")).collect()[0][0]\n",
    "    if engagements_others > 0:\n",
    "        silver_engagements_df = silver_engagements_df.union(\n",
    "            spark.createDataFrame([\n",
    "                Row(\n",
    "                    post_url = \"others\",\n",
    "                    post_publish_date = analytics_date,\n",
    "                    engagements = engagements_others,\n",
    "                    analytics_date = analytics_date,\n",
    "                    # ingestion_timestamp = silver_engagements_totals_df.select(\"ingestion_timestamp\").collect()[0].ingestion_timestamp,\n",
    "                    # source_file = silver_engagements_totals_df.select(\"source_file\").collect()[0].source_file,\n",
    "                    # source_file_timestamp = silver_engagements_totals_df.select(\"source_file_timestamp\").collect()[0].source_file_timestamp\n",
    "                )\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    if spark.catalog.tableExists(silver_engagements_table):\n",
    "        print(f\"Table {silver_engagements_table} exists, merging data...\")\n",
    "        delta_table = DeltaTable.forName(spark, silver_engagements_table)\n",
    "        (\n",
    "            delta_table.alias(\"t\")\n",
    "            .merge(\n",
    "                silver_engagements_df.alias(\"s\"),\n",
    "                \"t.post_url = s.post_url AND t.analytics_date = s.analytics_date\"\n",
    "            )\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Table {silver_engagements_table} does not exist, creating...\")\n",
    "        silver_engagements_df.write.format(\"delta\").saveAsTable(silver_engagements_table)\n",
    "    \n",
    "    # Drop the source table after processing\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    print(f\"Dropped table {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5123ec9d-bebd-48ed-b185-7f1a41d37329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Silver Layer: Fill in-between dates for engagement table with 0 values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a14304ad-9d23-425a-9478-31e6672f43b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, sequence, explode, to_date, min as spark_min, max as spark_max\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import sum\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# 1. define our constants\n",
    "try:\n",
    "    SILVER_CATALOG = get_valid_parameter(\"SILVER_CATALOG\")\n",
    "    SILVER_SCHEMA = get_valid_parameter(\"SILVER_SCHEMA\")\n",
    "    SILVER_ENGAGEMENTS_TABLE = dbusutils.widgets.get(\"SILVER_ENGAGEMENTS_TABLE\")\n",
    "except:\n",
    "    SILVER_CATALOG = \"silver\"\n",
    "    SILVER_SCHEMA = \"linkedin\"\n",
    "    SILVER_ENGAGEMENTS_TABLE = \"engagements\"\n",
    "\n",
    "silver_engagements_table = \\\n",
    "  f\"{SILVER_CATALOG}.{SILVER_SCHEMA}.{SILVER_ENGAGEMENTS_TABLE}\"\n",
    "\n",
    "\n",
    "def fill_missing_dates(df, date_col, group_cols, value_col):\n",
    "    # Get min and max dates\n",
    "    date_range = df.select(spark_min(date_col).alias(\"min_date\"), spark_max(date_col).alias(\"max_date\")).collect()[0]\n",
    "    min_date, max_date = date_range.min_date, date_range.max_date\n",
    "\n",
    "    # Create full date sequence\n",
    "    date_seq_df = spark.createDataFrame([(min_date, max_date)], [\"start\", \"end\"]) \\\n",
    "        .select(explode(sequence(col(\"start\"), col(\"end\"))).alias(date_col))\n",
    "\n",
    "    # Cross join with unique post_url and post_publish_date\n",
    "    unique_keys_df = df.select(*group_cols).distinct()\n",
    "    full_grid_df = unique_keys_df.crossJoin(date_seq_df)\n",
    "\n",
    "    # Left join to original df\n",
    "    filled_df = full_grid_df.join(\n",
    "        df,\n",
    "        on=group_cols + [date_col],\n",
    "        how=\"left\"\n",
    "    ).fillna({value_col: 0})\n",
    "\n",
    "    return filled_df\n",
    "\n",
    "# Fill missing dates for silver engagements\n",
    "silver_engagements_filled_df = fill_missing_dates(\n",
    "    spark.table(silver_engagements_table),\n",
    "    \"analytics_date\",\n",
    "    [\"post_url\", \"post_publish_date\"],\n",
    "    \"engagements\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6840877729571798,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver engagements consolidation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
