{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83d0e8f-90fe-465a-94a4-a08aa9bb420a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b42473a3-b00d-4c7f-b269-6c98ec250ba5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../utils')))\n",
    "\n",
    "    from pipeline_utils import get_valid_parameter\n",
    "except:\n",
    "    import re\n",
    "\n",
    "    def get_valid_parameter(parameter_key: str):\n",
    "        \"\"\"\n",
    "        Get a valid parameter value from the Databricks widgets.\n",
    "        Hardened to thwart SQL injection attacks.\n",
    "        \"\"\"\n",
    "        parameter_value = dbutils.widgets.get(parameter_key)\n",
    "        \n",
    "        # Parameter_value must be a string with only alphanumeric characters and underscores\n",
    "        if not re.fullmatch(r'[a-zA-Z0-9_]+', parameter_value):\n",
    "            raise ValueError(f\"Invalid parameter value for {parameter_key}: {parameter_value}\")\n",
    "        \n",
    "        # Disallow dangerous SQL keywords and patterns\n",
    "        forbidden_patterns = [\n",
    "            r'--', r';', r\"'\", r'\"', r'/\\*', r'\\*/', r'xp_', r'char\\(', r'nchar\\(', r'varchar\\(', r'\\balter\\b', r'\\bdrop\\b', r'\\binsert\\b', r'\\bdelete\\b', r'\\bupdate\\b', r'\\bselect\\b', r'\\bcreate\\b', r'\\bexec\\b', r'\\bunion\\b', r'\\bor\\b', r'\\band\\b'\n",
    "        ]\n",
    "        for pattern in forbidden_patterns:\n",
    "            if re.search(pattern, parameter_value, re.IGNORECASE):\n",
    "                raise ValueError(f\"Potentially dangerous value for {parameter_key}: {parameter_value} (pattern matched: {pattern})\")\n",
    "        return parameter_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "656d638d-1222-48e6-af54-98b1f56555ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 0. Set up constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e501b64-04ff-4e3e-9701-e08394155913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    LANDING_CATALOG = get_valid_parameter(\"LANDING_CATALOG\")\n",
    "    LANDING_SCHEMA = get_valid_parameter(\"LANDING_SCHEMA\")\n",
    "    LANDING_PATCH_VOLUME = get_valid_parameter(\"LANDING_PATCH_VOLUME\")\n",
    "    \n",
    "    PENDING_FOLDER = get_valid_parameter(\"PENDING_FOLDER\")\n",
    "    PROCESSED_FOLDER = get_valid_parameter(\"PROCESSED_FOLDER\")\n",
    "    ERRORS_FOLDER = get_valid_parameter(\"ERRORS_FOLDER\")\n",
    "    POST_PATCH_SUBFOLDER = get_valid_parameter(\"POST_PATCH_SUBFOLDER\")\n",
    "\n",
    "    BRONZE_CATALOG = get_valid_parameter(\"BRONZE_CATALOG\")\n",
    "    BRONZE_SCHEMA = get_valid_parameter(\"BRONZE_SCHEMA\")\n",
    "    BRONZE_POST_PATCH_TABLE = get_valid_parameter(\"BRONZE_POST_PATCH_TABLE\")\n",
    "\n",
    "    print(\"Loaded all widget values\")\n",
    "except:\n",
    "    LANDING_CATALOG = \"landing\"\n",
    "    LANDING_SCHEMA = \"linkedin\"\n",
    "    LANDING_PATCH_VOLUME = \"patch\"\n",
    "\n",
    "    PENDING_FOLDER = \"pending\"\n",
    "    PROCESSED_FOLDER = \"processed\"\n",
    "    ERRORS_FOLDER = \"errors\"\n",
    "    POST_PATCH_SUBFOLDER = \"posts\"\n",
    "\n",
    "    BRONZE_CATALOG = \"bronze\"\n",
    "    BRONZE_SCHEMA = \"linkedin\"\n",
    "    BRONZE_POST_DETAILS_TABLE = \"post_patch\"\n",
    "\n",
    "    print(\"Failed to load widget values, using default values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5803dd8-01de-48ea-9c35-7ac04785281b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# 1. set our input and output variables\n",
    "source_volume = \\\n",
    "  f\"/Volumes/{LANDING_CATALOG}/{LANDING_SCHEMA}/{LANDING_PATCH_VOLUME}/\"\n",
    "\n",
    "landing_pending_folder = f\"{source_volume}{PENDING_FOLDER}/{POST_PATCH_SUBFOLDER}/\"\n",
    "landing_processed_folder = f\"{source_volume}{PROCESSED_FOLDER}/{POST_PATCH_SUBFOLDER}/\"\n",
    "landing_errors_folder = f\"{source_volume}{ERRORS_FOLDER}/{POST_PATCH_SUBFOLDER}/\"\n",
    "\n",
    "bronze_post_patch_table = \\\n",
    "  f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_POST_PATCH_TABLE}\"\n",
    "\n",
    "# 2. execute the ingestion\n",
    "ingestion_timestamp = datetime.datetime.utcnow()\n",
    "\n",
    "# extract the list of files from the pending folder\n",
    "patch_files_info = [\n",
    "    (f.path, pd.to_datetime(f.modificationTime, unit='ms', utc=True).to_pydatetime()) \n",
    "    for f in dbutils.fs.ls(landing_pending_folder)\n",
    "]\n",
    "\n",
    "for file_path, file_timestamp in patch_files_info:\n",
    " \n",
    "  # extract filename from file path\n",
    "  filename = file_path.split('/')[-1]\n",
    "  \n",
    "  # define source and target paths for file\n",
    "  pending_path = landing_pending_folder + filename\n",
    "  processed_path = landing_processed_folder + filename\n",
    "  errors_path = landing_errors_folder + filename\n",
    "\n",
    "  # check if filename is of expected format\n",
    "  if filename.endswith('.xlsx'):\n",
    "      \n",
    "    # process valid filename\n",
    "    try:\n",
    "        # Read the Excel file into a pandas DataFrame\n",
    "        df = pd.read_excel(file_path)\n",
    "\n",
    "        # Define expected columns for the patch file\n",
    "        expected_columns = {\"post_url\", \"true_url\", \"title\", \"content\"}\n",
    "        if not expected_columns.issubset(set(df.columns)):\n",
    "            print(f\"Schema mismatch in {file_path}. Expected columns: {expected_columns}, found: {set(df.columns)}\")\n",
    "            dbutils.fs.mv(pending_path, errors_path)\n",
    "            continue\n",
    "        \n",
    "        # Convert pandas DataFrame to Spark DataFrame\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "\n",
    "        if spark.catalog.tableExists(bronze_post_patch_table):\n",
    "            delta_table = DeltaTable.forName(spark, bronze_post_patch_table)\n",
    "            delta_table.alias(\"t\").merge(\n",
    "                spark_df.alias(\"s\"),\n",
    "                \"t.post_url = s.post_url\"\n",
    "            ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "        else:\n",
    "            spark_df.write.format(\"delta\").saveAsTable(bronze_post_patch_table)\n",
    "\n",
    "        print(f\"Processed: Moving {pending_path} to {processed_path}\")\n",
    "        dbutils.fs.mv(pending_path, processed_path)\n",
    "\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      print(f\"Errors encountered: Moving {pending_path} to {errors_path}\")\n",
    "      dbutils.fs.mv(pending_path, errors_path)\n",
    "  else:\n",
    "    # move invalid filename to errors folder\n",
    "    try:\n",
    "      print(f\"Invalid filename: Moving {pending_path} to {errors_path}\")\n",
    "      dbutils.fs.mv(pending_path, errors_path)\n",
    "    except Exception as e:\n",
    "      print(f\"Failed to move file {pending_path}: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5468272325424032,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze post patch ingest",
   "widgets": {
    "BRONZE_CATALOG": {
     "currentValue": "bronze",
     "nuid": "54b95909-8662-4ee8-8726-66b542784d7c",
     "typedWidgetInfo": {
      "autoCreated": true,
      "defaultValue": "",
      "label": null,
      "name": "BRONZE_CATALOG",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "BRONZE_CATALOG",
      "options": {
       "autoCreated": true,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "BRONZE_POST_PATCH_TABLE": {
     "currentValue": "linkedin_patch",
     "nuid": "fd30c774-9cae-4349-9d53-e63f5449b990",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "BRONZE_POST_PATCH_TABLE",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "BRONZE_POST_PATCH_TABLE",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "BRONZE_SCHEMA": {
     "currentValue": "linkedin",
     "nuid": "e518a578-fcca-49e0-9b85-671a5b57a7a2",
     "typedWidgetInfo": {
      "autoCreated": true,
      "defaultValue": "",
      "label": null,
      "name": "BRONZE_SCHEMA",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "BRONZE_SCHEMA",
      "options": {
       "autoCreated": true,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
