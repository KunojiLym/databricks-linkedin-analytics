{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83d0e8f-90fe-465a-94a4-a08aa9bb420a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92ae32bd-9a17-46e2-9dc7-d317769926b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_valid_parameter(parameter_key: str):\n",
    "    \"\"\"\n",
    "    Get a valid parameter value from the Databricks widgets.\n",
    "    Hardened to thwart SQL injection attacks.\n",
    "    \"\"\"\n",
    "    parameter_value = dbutils.widgets.get(parameter_key)\n",
    "    \n",
    "    # Parameter_value must be a string with only alphanumeric characters and underscores\n",
    "    if not re.fullmatch(r'[a-zA-Z0-9_]+', parameter_value):\n",
    "        raise ValueError(f\"Invalid parameter value for {parameter_key}: {parameter_value}\")\n",
    "    \n",
    "    # Disallow dangerous SQL keywords and patterns\n",
    "    forbidden_patterns = [\n",
    "        r'--', r';', r\"'\", r'\"', r'/\\*', r'\\*/', r'xp_', r'char\\(', r'nchar\\(', r'varchar\\(', r'\\balter\\b', r'\\bdrop\\b', r'\\binsert\\b', r'\\bdelete\\b', r'\\bupdate\\b', r'\\bselect\\b', r'\\bcreate\\b', r'\\bexec\\b', r'\\bunion\\b', r'\\bor\\b', r'\\band\\b'\n",
    "    ]\n",
    "    for pattern in forbidden_patterns:\n",
    "        if re.search(pattern, parameter_value, re.IGNORECASE):\n",
    "            raise ValueError(f\"Potentially dangerous value for {parameter_key}: {parameter_value} (pattern matched: {pattern})\")\n",
    "    return parameter_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "656d638d-1222-48e6-af54-98b1f56555ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 0. Set up constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e501b64-04ff-4e3e-9701-e08394155913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    LINKEDIN_PROFILE_NAME = get_valid_parameter(\"LINKEDIN_PROFILE_NAME\")\n",
    "\n",
    "    LANDING_CATALOG = get_valid_parameter(\"LANDING_CATALOG\")\n",
    "    LANDING_SCHEMA = get_valid_parameter(\"LANDING_SCHEMA\")\n",
    "    LANDING_POSTS_VOLUME = get_valid_parameter(\"LANDING_POSTS_VOLUME\")\n",
    "    \n",
    "    PENDING_FOLDER = get_valid_parameter(\"PENDING_FOLDER\")\n",
    "    PROCESSED_FOLDER = get_valid_parameter(\"PROCESSED_FOLDER\")\n",
    "    ERRORS_FOLDER = get_valid_parameter(\"ERRORS_FOLDER\")\n",
    "\n",
    "    BRONZE_CATALOG = get_valid_parameter(\"BRONZE_CATALOG\")\n",
    "    BRONZE_SCHEMA = get_valid_parameter(\"BRONZE_SCHEMA\")\n",
    "    BRONZE_IMPRESSIONS_TABLE = get_valid_parameter(\"BRONZE_IMPRESSIONS_TABLE\")\n",
    "    BRONZE_POSTS_TABLE = get_valid_parameter(\"BRONZE_POSTS_TABLE\")\n",
    "    BRONZE_POST_DETAILS_TABLE = get_valid_parameter(\"BRONZE_POST_DETAILS_TABLE\")\n",
    "\n",
    "    print(\"Loaded all widget values\")\n",
    "except:\n",
    "    LINKEDIN_PROFILE_NAME = r\"[a-zA-Z0-9]+\" # use your own profile name here\n",
    "\n",
    "    LANDING_CATALOG = \"landing\"\n",
    "    LANDING_SCHEMA = \"linkedin\"\n",
    "    LANDING_POSTS_VOLUME = \"posts\"\n",
    "\n",
    "    PENDING_FOLDER = \"pending\"\n",
    "    PROCESSED_FOLDER = \"processed\"\n",
    "    ERRORS_FOLDER = \"errors\"\n",
    "\n",
    "    BRONZE_CATALOG = \"bronze\"\n",
    "    BRONZE_SCHEMA = \"linkedin\"\n",
    "    BRONZE_IMPRESSIONS_TABLE = \"impressions\"\n",
    "    BRONZE_POSTS_TABLE = \"posts\"\n",
    "    BRONZE_POST_DETAILS_TABLE = \"post_details\"\n",
    "\n",
    "    print(\"Failed to load widget values, using default values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f2f4290-3dd1-410a-b1c6-a59671478962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Helper functions for fetching and parsing LinkedIn post HTML content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e440da-fcfb-448d-a950-5acaff52926e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions for fetching and parsing LinkedIn post HTML content\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def get_html_content(url: str, max_retries: int = 5, base_delay: float = 5.0):\n",
    "    \"\"\"\n",
    "    Fetches HTML content from a LinkedIn post URL with exponential backoff.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Fetching HTML content from URL: {url} (Attempt {attempt + 1})\")\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            print(f\"Successfully fetched HTML content from {url}\")\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                delay = base_delay * (2 ** attempt)\n",
    "                print(f\"Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(f\"Failed to fetch HTML content from {url} after {max_retries} attempts.\")\n",
    "                raise e\n",
    "\n",
    "def get_content(html_content):\n",
    "    \"\"\"\n",
    "    Extracts post content from HTML using BeautifulSoup.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    content_wrapper = soup.find('p', class_=\"attributed-text-segment-list__content\")\n",
    "    if content_wrapper:\n",
    "        return content_wrapper.get_text()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f467588d-adcc-49be-a6ec-b567fd20405f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Create patch table for LinkedIn posts if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "766c98db-d7dc-45d7-a966-fcd35f7f3d75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS IDENTIFIER (:BRONZE_CATALOG || '.' || :BRONZE_SCHEMA || '.' || :BRONZE_POST_PATCH_TABLE) (\n",
    "  post_url STRING,\n",
    "  true_url STRING,\n",
    "  title STRING,\n",
    "  content STRING\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5640436-c647-4bce-a10e-5859de467e1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Ingest post metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75235a8-4062-4f7e-ad06-1620a0e03ed6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract posts data using post_url from all impressions tables and save to bronze posts table\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, row_number, desc\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "bronze_impressions_table = f'{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_IMPRESSIONS_TABLE}'\n",
    "bronze_posts_table = f'{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_POSTS_TABLE}'\n",
    "\n",
    "# List all impressions staging tables in the bronze.linkedin schema\n",
    "daily_impressions_tables = [table.name for table in spark.catalog.listTables(f'{BRONZE_CATALOG}.{BRONZE_SCHEMA}') if table.name.startswith(BRONZE_IMPRESSIONS_TABLE)]\n",
    "\n",
    "print(\"Loading daily impressions tables to extract posts data...\")\n",
    "# Read each table and select the relevant columns\n",
    "columns_to_select = [\n",
    "  \"post_url\", \"post_publish_date\", \n",
    "  \"ingestion_timestamp\", \"source_file\", \"source_file_timestamp\"\n",
    "]\n",
    "dfs = [spark.read.table(f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{table}\").select(*columns_to_select) for table in daily_impressions_tables]\n",
    "\n",
    "# Union all dataframes and remove duplicates\n",
    "if dfs:\n",
    "    impressions_df = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        impressions_df = impressions_df.unionByName(df)\n",
    "    impressions_df = impressions_df.withColumn(\n",
    "        \"row_num\",\n",
    "        row_number().over(\n",
    "            Window.partitionBy(\"post_url\", \"post_publish_date\").orderBy(desc(\"ingestion_timestamp\"))\n",
    "        )\n",
    "    ).filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "    impressions_pd = impressions_df.toPandas()[columns_to_select]\n",
    "    impressions_pd['post_publish_date'] = pd.to_datetime(impressions_pd['post_publish_date'])\n",
    "else:\n",
    "    impressions_pd = pd.DataFrame(columns=columns_to_select)\n",
    "\n",
    "if not impressions_pd.empty:\n",
    "    # Fetch HTML content for each post URL\n",
    "    impressions_pd['html_content'] = impressions_pd['post_url'].apply(get_html_content)\n",
    "    # Extract link, title, and content from the HTML content\n",
    "    impressions_pd['link'] = impressions_pd['html_content'].apply(lambda x: BeautifulSoup(x, \"html.parser\").find('link').get('href'))\n",
    "    impressions_pd['title'] = impressions_pd['html_content'].apply(lambda x: BeautifulSoup(x, \"html.parser\").find(\"head\").get_text(strip=True).split('|')[0].strip())\n",
    "    impressions_pd['content'] = impressions_pd['html_content'].apply(get_content)\n",
    "\n",
    "    impressions_spark_df = spark.createDataFrame(impressions_pd)\n",
    "    if not spark.catalog.tableExists(bronze_posts_table):\n",
    "        print(f\"Creating {bronze_posts_table} table...\")\n",
    "        impressions_spark_df.write.format(\"delta\").saveAsTable(bronze_posts_table)\n",
    "    else:\n",
    "        print(f\"Appending new posts to {bronze_posts_table} table...\")\n",
    "        impressions_spark_df.write.format(\"delta\").mode(\"append\").saveAsTable(bronze_posts_table)\n",
    "else:\n",
    "    print(\"No new posts found based on impressions tables.\")\n",
    "\n",
    "# Display sample of posts data for verification\n",
    "posts_sample_df = spark.read.table(bronze_posts_table).limit(10)\n",
    "display(posts_sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "107f9a3c-4adf-4aa3-b997-23e92f6af616",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Ingest post analytics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cc37617-4194-493b-b9cf-9c7826fb3c38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# 1. set our input and output variables\n",
    "source_volume = \\\n",
    "  f\"/Volumes/{LANDING_CATALOG}/{LANDING_SCHEMA}/{LANDING_POSTS_VOLUME}/\"\n",
    "\n",
    "landing_pending_folder = f\"{source_volume}{PENDING_FOLDER}/\"\n",
    "landing_processed_folder = f\"{source_volume}{PROCESSED_FOLDER}/\"\n",
    "landing_errors_folder = f\"{source_volume}{ERRORS_FOLDER}/\"\n",
    "\n",
    "bronze_post_details_table = \\\n",
    "  f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_POST_DETAILS_TABLE}\"\n",
    "\n",
    "# 2. execute the ingestion\n",
    "ingestion_timestamp = datetime.datetime.utcnow()\n",
    "\n",
    "# extract the list of files from the pending folder\n",
    "post_files_info = [\n",
    "    (f.path, pd.to_datetime(f.modificationTime, unit='ms', utc=True).to_pydatetime()) \n",
    "    for f in dbutils.fs.ls(landing_pending_folder)\n",
    "]\n",
    "\n",
    "for file_path, file_timestamp in post_files_info:\n",
    " \n",
    "  # extract filename from file path\n",
    "  filename = file_path.split('/')[-1]\n",
    "  \n",
    "  # define source and target paths for file\n",
    "  pending_path = landing_pending_folder + filename\n",
    "  processed_path = landing_processed_folder + filename\n",
    "  errors_path = landing_errors_folder + filename\n",
    "\n",
    "  # check if filename is of expected format\n",
    "  if re.search(\n",
    "    r'PostAnalytics_' + LINKEDIN_PROFILE_NAME + r'_\\d+(?: \\(\\d+\\))?\\.xlsx', filename\n",
    "  ):\n",
    "      \n",
    "    # process valid filename\n",
    "    try:\n",
    "      # read totals from xlsx file\n",
    "      print(f\"Extracting post URL and timestamp from PERFORMANCE sheet in {pending_path}\")\n",
    "      post_details_raw_df = pd.read_excel(\n",
    "        pending_path, \n",
    "        sheet_name=\"PERFORMANCE\", \n",
    "        header=None,\n",
    "        names=[\"key\", \"value\"],\n",
    "      ).dropna(how='all')\n",
    "\n",
    "      # Extract timestamp. post URL and date of analytics\n",
    "      transposed_post_details_df = post_details_raw_df.dropna().set_index('key').transpose()\n",
    "      post_timedelta_str = transposed_post_details_df[\"Post Publish Time\"].iloc[0]\n",
    "\n",
    "      # Extract date range of analytics from specific string pattern\n",
    "      df_with_date_range = post_details_raw_df[post_details_raw_df['key'].str.contains(\n",
    "          \" Highlights \", na=False\n",
    "        )]['key']\n",
    "      if df_with_date_range.empty:\n",
    "        print(\"Cannot detect analytics date\")\n",
    "        date_start_str = transposed_post_details_df[\"Post Date\"].iloc[0]\n",
    "        date_end_str = None\n",
    "      else:\n",
    "        date_range_str = \\\n",
    "          df_with_date_range.str.split(\" Highlights \").iloc[0][1]\n",
    "        [date_start_str, date_end_str] = date_range_str.split(\" to \")\n",
    "\n",
    "      post_timestamp = pd.to_datetime(date_start_str + ' ' + timedelta_str + ' UTC').to_pydatetime()\n",
    "\n",
    "      post_url = transposed_post_details_df[\"Post URL\"].iloc[0]\n",
    "      if date_end_str:\n",
    "        analytics_date = pd.to_datetime(date_end_str).to_pydatetime().date()\n",
    "      else:\n",
    "        analytics_date = None\n",
    "\n",
    "      post_top_demographics_df = pd.read_excel(\n",
    "        pending_path, \n",
    "        sheet_name=\"TOP DEMOGRAPHICS\", \n",
    "      ).dropna()\n",
    "      post_top_demographics_df.columns = post_top_demographics_df.columns.str.lower().str.replace('%', 'percent')\n",
    "\n",
    "      # Fetch HTML content for each post URL\n",
    "      print(f\"Fetching HTML content and link for {post_url}\")\n",
    "      html_content = get_html_content(post_url)\n",
    "      # Extract link from the HTML content\n",
    "      link = BeautifulSoup(html_content, \"html.parser\").find('link').get('href')\n",
    "      \n",
    "      print(f\"Preparing post details record for {link}\")\n",
    "      post_details_record = Row(\n",
    "          post_url=post_url, \n",
    "          link=link,\n",
    "          post_timestamp=post_timestamp,\n",
    "          analytics_date=analytics_date,\n",
    "          performance_metrics=str(transposed_post_details_df.to_dict()),\n",
    "          demographics_metrics=str(post_top_demographics_df.to_dict()),\n",
    "          ingestion_timestamp=ingestion_timestamp,\n",
    "          source_file=file_path,\n",
    "          source_file_timestamp=file_timestamp,\n",
    "      )\n",
    "\n",
    "      # Write post_details_record to Delta table with upsert logic based on latest analytics date\n",
    "      print(f\"Writing to {bronze_post_details_table}: {post_details_record}\")\n",
    "      post_details_spark_df = spark.createDataFrame([post_details_record])\n",
    "      if spark.catalog.tableExists(bronze_post_details_table):\n",
    "          delta_table = DeltaTable.forName(spark, bronze_post_details_table)\n",
    "          delta_table.alias(\"t\").merge(\n",
    "              post_details_spark_df.alias(\"s\"),\n",
    "              \"t.post_url = s.post_url and t.analytics_date = s.analytics_date\"\n",
    "          ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "      else:\n",
    "          post_details_spark_df.write.format(\"delta\").saveAsTable(\n",
    "            bronze_post_details_table\n",
    "          )\n",
    "\n",
    "      print(f\"Processed: Moving {pending_path} to {processed_path}\")\n",
    "      dbutils.fs.mv(pending_path, processed_path)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      print(f\"Errors encountered: Moving {pending_path} to {errors_path}\")\n",
    "      dbutils.fs.mv(pending_path, errors_path)\n",
    "  else:\n",
    "    # move invalid filename to errors folder\n",
    "    try:\n",
    "      print(f\"Invalid filename: Moving {pending_path} to {errors_path}\")\n",
    "      dbutils.fs.mv(pending_path, errors_path)\n",
    "    except Exception as e:\n",
    "      print(f\"Failed to move file {pending_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf7ccfb6-2758-48ea-8988-61852f6eeef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS IDENTIFIER (:BRONZE_CATALOG || '.' || :BRONZE_SCHEMA || '.' || :BRONZE_POST_DETAILS_TABLE) (\n",
    "  post_url STRING,\n",
    "  link STRING,\n",
    "  post_timestamp TIMESTAMP,\n",
    "  analytics_date DATE,\n",
    "  performance_metrics STRING,\n",
    "  demographics_metrics STRING,\n",
    "  ingestion_timestamp TIMESTAMP\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5468272325424032,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze post ingest",
   "widgets": {
    "BRONZE_CATALOG": {
     "currentValue": "bronze",
     "nuid": "54b95909-8662-4ee8-8726-66b542784d7c",
     "typedWidgetInfo": {
      "autoCreated": true,
      "defaultValue": "",
      "label": null,
      "name": "BRONZE_CATALOG",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "BRONZE_CATALOG",
      "options": {
       "widgetType": "text",
       "autoCreated": true,
       "validationRegex": null
      }
     }
    },
    "BRONZE_POST_DETAILS_TABLE": {
     "currentValue": "",
     "nuid": "90559d13-895c-4d26-bc67-5773e20b43a2",
     "typedWidgetInfo": {
      "autoCreated": true,
      "defaultValue": "",
      "label": null,
      "name": "BRONZE_POST_DETAILS_TABLE",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "BRONZE_POST_DETAILS_TABLE",
      "options": {
       "widgetType": "text",
       "autoCreated": true,
       "validationRegex": null
      }
     }
    },
    "BRONZE_POST_PATCH_TABLE": {
     "currentValue": "linkedin_patch",
     "nuid": "fd30c774-9cae-4349-9d53-e63f5449b990",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "BRONZE_POST_PATCH_TABLE",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "BRONZE_POST_PATCH_TABLE",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "BRONZE_SCHEMA": {
     "currentValue": "linkedin",
     "nuid": "e518a578-fcca-49e0-9b85-671a5b57a7a2",
     "typedWidgetInfo": {
      "autoCreated": true,
      "defaultValue": "",
      "label": null,
      "name": "BRONZE_SCHEMA",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "BRONZE_SCHEMA",
      "options": {
       "widgetType": "text",
       "autoCreated": true,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
