{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83d0e8f-90fe-465a-94a4-a08aa9bb420a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1cb63d3-0201-4161-8cc3-9c67c76f975d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_valid_parameter(parameter_key: str):\n",
    "    \"\"\"\n",
    "    Get a valid parameter value from the Databricks widgets.\n",
    "    Hardened to thwart SQL injection attacks.\n",
    "    \"\"\"\n",
    "    parameter_value = dbutils.widgets.get(parameter_key)\n",
    "    \n",
    "    # Parameter_value must be a string with only alphanumeric characters and underscores\n",
    "    if not re.fullmatch(r'[a-zA-Z0-9_]+', parameter_value):\n",
    "        raise ValueError(f\"Invalid parameter value for {parameter_key}: {parameter_value}\")\n",
    "    \n",
    "    # Disallow dangerous SQL keywords and patterns\n",
    "    forbidden_patterns = [\n",
    "        r'--', r';', r\"'\", r'\"', r'/\\*', r'\\*/', r'xp_', r'char\\(', r'nchar\\(', r'varchar\\(', r'\\balter\\b', r'\\bdrop\\b', r'\\binsert\\b', r'\\bdelete\\b', r'\\bupdate\\b', r'\\bselect\\b', r'\\bcreate\\b', r'\\bexec\\b', r'\\bunion\\b', r'\\bor\\b', r'\\band\\b'\n",
    "    ]\n",
    "    for pattern in forbidden_patterns:\n",
    "        if re.search(pattern, parameter_value, re.IGNORECASE):\n",
    "            raise ValueError(f\"Potentially dangerous value for {parameter_key}: {parameter_value} (pattern matched: {pattern})\")\n",
    "    return parameter_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f8de9a-5588-4a47-ac94-af1de9c23560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Process daily analytics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22257c97-06d9-4924-b2bb-3160e74da7da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# 1. define our constants\n",
    "try:\n",
    "  LINKEDIN_PROFILE_NAME = get_valid_parameter(\"LINKEDIN_PROFILE_NAME\")\n",
    "\n",
    "  LANDING_CATALOG = get_valid_parameter(\"LANDING_CATALOG\")\n",
    "  LANDING_SCHEMA = get_valid_parameter(\"LANDING_SCHEMA\")\n",
    "  LANDING_DAILY_VOLUME = get_valid_parameter(\"LANDING_DAILY_VOLUME\")\n",
    "\n",
    "  PENDING_FOLDER = get_valid_parameter(\"PENDING_FOLDER\")\n",
    "  PROCESSED_FOLDER = get_valid_parameter(\"PROCESSED_FOLDER\")\n",
    "  ERRORS_FOLDER = get_valid_parameter(\"ERRORS_FOLDER\")\n",
    "\n",
    "  BRONZE_CATALOG = get_valid_parameter(\"BRONZE_CATALOG\")\n",
    "  BRONZE_SCHEMA = get_valid_parameter(\"BRONZE_SCHEMA\")\n",
    "  BRONZE_DISCOVERY_TABLE = get_valid_parameter(\"BRONZE_DISCOVERY_TABLE\")\n",
    "  BRONZE_TOTALS_TABLE = get_valid_parameter(\"BRONZE_TOTALS_TABLE\")\n",
    "  BRONZE_FOLLOWERS_TABLE = get_valid_parameter(\"BRONZE_FOLLOWERS_TABLE\")\n",
    "\n",
    "  BRONZE_IMPRESSIONS_TABLE = get_valid_parameter(\"BRONZE_IMPRESSIONS_TABLE\")\n",
    "  BRONZE_ENGAGEMENTS_TABLE = get_valid_parameter(\"BRONZE_ENGAGEMENTS_TABLE\")\n",
    "\n",
    "  print(\"Loaded all widget values\")\n",
    "except Exception as e:\n",
    "  LINKEDIN_PROFILE_NAME = r\"[a-zA-Z0-9]+\" # use your own profile name here\n",
    "\n",
    "  LANDING_CATALOG = \"landing\"\n",
    "  LANDING_SCHEMA = \"linkedin\"\n",
    "  LANDING_DAILY_VOLUME = \"content_daily\"\n",
    "\n",
    "  PENDING_FOLDER = \"pending\"\n",
    "  PROCESSED_FOLDER = \"processed\"\n",
    "  ERRORS_FOLDER = \"errors\"\n",
    "\n",
    "  BRONZE_CATALOG = \"bronze\"\n",
    "  BRONZE_SCHEMA = \"linkedin\"\n",
    "  BRONZE_DISCOVERY_TABLE = \"discovery\"\n",
    "  BRONZE_TOTALS_TABLE = \"totals\"\n",
    "  BRONZE_FOLLOWERS_TABLE = \"followers\"\n",
    "\n",
    "  BRONZE_IMPRESSIONS_TABLE = \"impressions\"\n",
    "  BRONZE_ENGAGEMENTS_TABLE = \"engagements\"\n",
    "\n",
    "  print(f\"Failed to load widget values ({e}), using default values\")\n",
    "\n",
    "# 2. set our input and output variables\n",
    "source_volume = \\\n",
    "  f\"/Volumes/{LANDING_CATALOG}/{LANDING_SCHEMA}/{LANDING_DAILY_VOLUME}/\"\n",
    "\n",
    "landing_pending_folder = f\"{source_volume}{PENDING_FOLDER}/\"\n",
    "landing_processed_folder = f\"{source_volume}{PROCESSED_FOLDER}/\"\n",
    "landing_errors_folder = f\"{source_volume}{ERRORS_FOLDER}/\"\n",
    "\n",
    "bronze_totals_table = \\\n",
    "  f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_TOTALS_TABLE}\"\n",
    "bronze_discovery_table = \\\n",
    "  f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_DISCOVERY_TABLE}\"\n",
    "\n",
    "bronze_followers_table = \\\n",
    "  f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_FOLLOWERS_TABLE}\"\n",
    "\n",
    "bronze_impressions_table_prefix = \\\n",
    "    f'{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_IMPRESSIONS_TABLE}'\n",
    "bronze_engagements_table_prefix = \\\n",
    "    f'{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_ENGAGEMENTS_TABLE}'\n",
    "\n",
    "# 3. execute the ingestion\n",
    "ingestion_timestamp = pd.Timestamp.utcnow()\n",
    "\n",
    "# extract the list of files from the pending folder\n",
    "daily_files_info = [\n",
    "    (f.path, pd.to_datetime(f.modificationTime, unit='ms', utc=True)) \n",
    "    for f in dbutils.fs.ls(landing_pending_folder)\n",
    "]\n",
    "\n",
    "# get current count of tables in bronze schema\n",
    "table_count_in_bronze_schema = spark.sql(\n",
    "    f\"SHOW TABLES IN {BRONZE_CATALOG}.{BRONZE_SCHEMA}\"\n",
    ").count()\n",
    "dates_processed = []\n",
    "\n",
    "for file_path, file_timestamp in daily_files_info:\n",
    "\n",
    "  tables_to_create = 0  \n",
    "  \n",
    "  # extract filename from file path\n",
    "  filename = file_path.split('/')[-1]\n",
    "  \n",
    "  # define source and target paths for file\n",
    "  pending_path = landing_pending_folder + filename\n",
    "  processed_path = landing_processed_folder + filename\n",
    "  errors_path = landing_errors_folder + filename\n",
    "\n",
    "  # check if filename is of expected format (from and to date are the same)\n",
    "  if re.search(\n",
    "    r'Content_(\\d{4}-\\d{2}-\\d{2})_\\1_' \n",
    "    + LINKEDIN_PROFILE_NAME + r'\\.xlsx', filename\n",
    "  ):\n",
    "      \n",
    "    # extract date of analytics and append it to staging table names\n",
    "    analytics_date_str = re.search(\n",
    "      r'Content_(\\d{4}-\\d{2}-\\d{2})_\\1_', filename\n",
    "    ).group(1)\n",
    "    analytics_date = pd.to_datetime(analytics_date_str).date()\n",
    "    bronze_engagements_table = \\\n",
    "      f\"{bronze_engagements_table_prefix}_{\n",
    "          analytics_date_str.replace('-', '_')\n",
    "        }\"\n",
    "    bronze_impressions_table = \\\n",
    "      f\"{bronze_impressions_table_prefix}_{\n",
    "          analytics_date_str.replace('-', '_')\n",
    "        }\"\n",
    "\n",
    "    # check if bronze_engagement_table exists in bronze schema\n",
    "    if not spark.catalog.tableExists(bronze_engagements_table):\n",
    "      tables_to_create += 1\n",
    "\n",
    "    # check if bronze_impressions_table exists in bronze schema\n",
    "    if not spark.catalog.tableExists(bronze_impressions_table):\n",
    "      tables_to_create += 1\n",
    "\n",
    "    # exit if too many tables in bronze schema (specific to Databricks Free Edition)\n",
    "    if table_count_in_bronze_schema + tables_to_create > 100:\n",
    "        print(\"Too many tables in bronze schema. Please process and clear staging tables before rerunning.\")\n",
    "        break\n",
    "\n",
    "    # process valid filename\n",
    "    try:\n",
    "      try:\n",
    "        # read members reached from xlsx file\n",
    "        print(f\"Processing DISCOVERY sheet in {pending_path}\")\n",
    "        discovery_df = pd.read_excel(\n",
    "          pending_path, \n",
    "          sheet_name=\"DISCOVERY\", \n",
    "        ).set_index(\"Overall Performance\").transpose().dropna().reset_index(drop=True)\n",
    "        discovery_df.columns = discovery_df.columns.str.lower().str.replace(' ', '_') \n",
    "\n",
    "        discovery_df = discovery_df[['members_reached']]\n",
    "        discovery_df['date'] = analytics_date\n",
    "        discovery_df['ingestion_timestamp'] = ingestion_timestamp\n",
    "        discovery_df['source_file'] = filename\n",
    "        discovery_df['source_file_timestamp'] = file_timestamp\n",
    "\n",
    "        # Write members reached to Delta table with upsert logic\n",
    "        print(f\"Writing to {bronze_discovery_table}\")\n",
    "        if not discovery_df.empty:\n",
    "          if spark.catalog.tableExists(bronze_discovery_table):\n",
    "            delta_table = DeltaTable.forName(spark, bronze_discovery_table)\n",
    "            delta_table.alias(\"t\").merge(\n",
    "              spark.createDataFrame(discovery_df).alias(\"s\"),\n",
    "              \"t.date = s.date\"\n",
    "            ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "          else:\n",
    "            spark.createDataFrame(discovery_df).write.format(\"delta\").saveAsTable(\n",
    "              bronze_discovery_table\n",
    "            )\n",
    "      except Exception as e:\n",
    "        print(f\"Error writing to {bronze_discovery_table}: {e}\")\n",
    "        exit()\n",
    "\n",
    "      # read totals from xlsx file\n",
    "      print(f\"Processing ENGAGEMENT sheet in {pending_path}\")\n",
    "      totals_df = pd.read_excel(\n",
    "        pending_path, \n",
    "        sheet_name=\"ENGAGEMENT\", \n",
    "        parse_dates=[\"Date\"]\n",
    "      ).dropna()\n",
    "      totals_df.columns = totals_df.columns.str.lower().str.replace(' ', '_') \n",
    "      totals_df['ingestion_timestamp'] = ingestion_timestamp\n",
    "      totals_df['source_file'] = filename\n",
    "      totals_df['source_file_timestamp'] = file_timestamp\n",
    "      \n",
    "      # Write totals to Delta table with upsert logic\n",
    "      print(f\"Writing to {bronze_totals_table}\")\n",
    "      if not totals_df.empty:\n",
    "        if spark.catalog.tableExists(bronze_totals_table):\n",
    "          delta_table = DeltaTable.forName(spark, bronze_totals_table)\n",
    "          delta_table.alias(\"t\").merge(\n",
    "            spark.createDataFrame(totals_df).alias(\"s\"),\n",
    "            \"t.date = s.date\"\n",
    "          ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "        else:\n",
    "          spark.createDataFrame(totals_df).write.format(\"delta\").saveAsTable(\n",
    "            bronze_totals_table\n",
    "          )\n",
    "\n",
    "      # read followers from xlsx file\n",
    "      print(f\"Processing FOLLOWERS sheet in {pending_path}\")\n",
    "      followers_df = pd.read_excel(\n",
    "        pending_path, \n",
    "        sheet_name=\"FOLLOWERS\", \n",
    "        parse_dates=[\"Date\"],\n",
    "        skiprows=2\n",
    "      ).dropna()\n",
    "      followers_df.columns = followers_df.columns.str.lower().str.replace(' ', '_') \n",
    "      followers_df['ingestion_timestamp'] = ingestion_timestamp\n",
    "      followers_df['source_file'] = filename\n",
    "      followers_df['source_file_timestamp'] = file_timestamp\n",
    "      \n",
    "      # Write followers to Delta table with upsert logic\n",
    "      print(f\"Writing to {bronze_followers_table}\")\n",
    "      if not followers_df.empty:\n",
    "        if spark.catalog.tableExists(bronze_followers_table):\n",
    "          delta_table = DeltaTable.forName(spark, bronze_followers_table)\n",
    "          delta_table.alias(\"t\").merge(\n",
    "            spark.createDataFrame(followers_df).alias(\"s\"),\n",
    "            \"t.date = s.date\"\n",
    "          ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "        else:\n",
    "          spark.createDataFrame(followers_df).write.format(\"delta\").saveAsTable(\n",
    "            bronze_followers_table\n",
    "          )\n",
    "\n",
    "      # read top posts from xlsx file  \n",
    "      print(f\"Processing TOP POSTS sheet in {pending_path}\")\n",
    "      topposts_df = pd.read_excel(pending_path, sheet_name=\"TOP POSTS\", skiprows=2)\n",
    "\n",
    "      engagements_df = topposts_df.iloc[:, :3].dropna()\n",
    "      if not engagements_df.empty:  \n",
    "        engagements_df.columns = engagements_df.columns.str.replace(' ', '_').str.lower()\n",
    "        engagements_df['analytics_date'] = analytics_date\n",
    "        engagements_df['ingestion_timestamp'] = ingestion_timestamp\n",
    "        engagements_df['source_file'] = filename\n",
    "        engagements_df['source_file_timestamp'] = file_timestamp\n",
    "\n",
    "      impressions_df = topposts_df.iloc[:, 4:].dropna()\n",
    "      if not impressions_df.empty:\n",
    "        impressions_df.columns = impressions_df.columns.str.replace('.1', '', regex=False).str.replace(' ', '_').str.lower()\n",
    "        impressions_df['analytics_date'] = analytics_date\n",
    "        impressions_df['ingestion_timestamp'] = ingestion_timestamp\n",
    "        impressions_df['source_file'] = filename\n",
    "        impressions_df['source_file_timestamp'] = file_timestamp\n",
    "\n",
    "      # Write engagements to Delta staging table, overwrite existing      \n",
    "      if not engagements_df.empty:\n",
    "        print(f\"Writing to {bronze_engagements_table}\")\n",
    "        spark.createDataFrame(engagements_df).write.mode(\"overwrite\").saveAsTable(\n",
    "          bronze_engagements_table\n",
    "        )\n",
    "      else:\n",
    "        print(f\"Skipped writing empty dataset to {bronze_engagements_table}\")\n",
    "\n",
    "      # Write impressions to Delta staging table, overwrite existing\n",
    "      \n",
    "      if not impressions_df.empty:\n",
    "        print(f\"Writing to {bronze_impressions_table}\")\n",
    "        spark.createDataFrame(impressions_df).write.mode(\"overwrite\").saveAsTable(\n",
    "          bronze_impressions_table\n",
    "        )\n",
    "      else:\n",
    "        print(f\"Skipped writing empty dataset to {bronze_impressions_table}\")\n",
    "\n",
    "      # update counters for dates processed and tables in bronze schema\n",
    "      dates_processed.append(analytics_date_str)  \n",
    "      table_count_in_bronze_schema += tables_to_create\n",
    "\n",
    "      print(f\"Processed: Moving {pending_path} to {processed_path}\")\n",
    "      dbutils.fs.mv(pending_path, processed_path)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      print(f\"Errors encountered: Moving {pending_path} to {errors_path}\")\n",
    "      dbutils.fs.mv(pending_path, errors_path)\n",
    "  else:\n",
    "    # move invalid filename to errors folder\n",
    "    try:\n",
    "      print(f\"Invalid filename: Moving {pending_path} to {errors_path}\")\n",
    "      dbutils.fs.mv(pending_path, errors_path)\n",
    "    except Exception as e:\n",
    "      print(f\"Failed to move file {pending_path}: {e}\")\n",
    "\n",
    "if len(dates_processed) == 0:\n",
    "    print(\"No dates processed.\")\n",
    "else:\n",
    "    print(f\"Dates processed: {dates_processed}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6840877729571798,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze daily ingest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
