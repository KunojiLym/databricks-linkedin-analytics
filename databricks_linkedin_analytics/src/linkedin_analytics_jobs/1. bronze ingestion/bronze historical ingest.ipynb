{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83d0e8f-90fe-465a-94a4-a08aa9bb420a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75d46101-9cd1-437b-adf0-1249dde4d980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../utils')))\n",
    "\n",
    "    from pipeline_utils import get_valid_parameter\n",
    "except:\n",
    "    import re\n",
    "\n",
    "    def get_valid_parameter(parameter_key: str):\n",
    "        \"\"\"\n",
    "        Get a valid parameter value from the Databricks widgets.\n",
    "        Hardened to thwart SQL injection attacks.\n",
    "        \"\"\"\n",
    "        parameter_value = dbutils.widgets.get(parameter_key)\n",
    "        \n",
    "        # Parameter_value must be a string with only alphanumeric characters and underscores\n",
    "        if not re.fullmatch(r'[a-zA-Z0-9_]+', parameter_value):\n",
    "            raise ValueError(f\"Invalid parameter value for {parameter_key}: {parameter_value}\")\n",
    "        \n",
    "        # Disallow dangerous SQL keywords and patterns\n",
    "        forbidden_patterns = [\n",
    "            r'--', r';', r\"'\", r'\"', r'/\\*', r'\\*/', r'xp_', r'char\\(', r'nchar\\(', r'varchar\\(', r'\\balter\\b', r'\\bdrop\\b', r'\\binsert\\b', r'\\bdelete\\b', r'\\bupdate\\b', r'\\bselect\\b', r'\\bcreate\\b', r'\\bexec\\b', r'\\bunion\\b', r'\\bor\\b', r'\\band\\b'\n",
    "        ]\n",
    "        for pattern in forbidden_patterns:\n",
    "            if re.search(pattern, parameter_value, re.IGNORECASE):\n",
    "                raise ValueError(f\"Potentially dangerous value for {parameter_key}: {parameter_value} (pattern matched: {pattern})\")\n",
    "        return parameter_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9e5a51a-adf3-4b46-a1db-10736e918b56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Process historical analytics data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d452bcde-c52a-41a7-a2ae-14f2c2b107e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# 1. define our constants\n",
    "try:\n",
    "  LINKEDIN_PROFILE_NAME = get_valid_parameter(\"LINKEDIN_PROFILE_NAME\")\n",
    "\n",
    "  LANDING_CATALOG = get_valid_parameter(\"LANDING_CATALOG\")\n",
    "  LANDING_SCHEMA = get_valid_parameter(\"LANDING_SCHEMA\")\n",
    "  LANDING_DAILY_VOLUME = get_valid_parameter(\"LANDING_DAILY_VOLUME\")\n",
    "\n",
    "  PENDING_FOLDER = get_valid_parameter(\"PENDING_FOLDER\")\n",
    "  PROCESSED_FOLDER = get_valid_parameter(\"PROCESSED_FOLDER\")\n",
    "  ERRORS_FOLDER = get_valid_parameter(\"ERRORS_FOLDER\")\n",
    "\n",
    "  BRONZE_CATALOG = get_valid_parameter(\"BRONZE_CATALOG\")\n",
    "  BRONZE_SCHEMA = get_valid_parameter(\"BRONZE_SCHEMA\")\n",
    "  BRONZE_TOTALS_TABLE = get_valid_parameter(\"BRONZE_TOTALS_TABLE\")\n",
    "  BRONZE_FOLLOWERS_TABLE = get_valid_parameter(\"BRONZE_FOLLOWERS_TABLE\")\n",
    "\n",
    "  print(\"Loaded all widget values\")\n",
    "except:\n",
    "  LINKEDIN_PROFILE_NAME = r\"[a-zA-Z0-9]+\" # use your own profile name here\n",
    "\n",
    "  LANDING_CATALOG = \"landing\"\n",
    "  LANDING_SCHEMA = \"linkedin\"\n",
    "  LANDING_DAILY_VOLUME = \"content_historical\"\n",
    "\n",
    "  PENDING_FOLDER = \"pending\"\n",
    "  PROCESSED_FOLDER = \"processed\"\n",
    "  ERRORS_FOLDER = \"errors\"\n",
    "\n",
    "  BRONZE_CATALOG = \"bronze\"\n",
    "  BRONZE_SCHEMA = \"linkedin\"\n",
    "  BRONZE_TOTALS_TABLE = \"totals\"\n",
    "  BRONZE_FOLLOWERS_TABLE = \"followers\"\n",
    "\n",
    "  print(\"Failed to load widget values, using default values\")\n",
    "\n",
    "# 2. set our input and output variables\n",
    "source_volume = \\\n",
    "  f\"/Volumes/{LANDING_CATALOG}/{LANDING_SCHEMA}/{LANDING_DAILY_VOLUME}/\"\n",
    "\n",
    "landing_pending_folder = f\"{source_volume}{PENDING_FOLDER}/\"\n",
    "landing_processed_folder = f\"{source_volume}{PROCESSED_FOLDER}/\"\n",
    "landing_errors_folder = f\"{source_volume}{ERRORS_FOLDER}/\"\n",
    "\n",
    "bronze_totals_table = \\\n",
    "  f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_TOTALS_TABLE}\"\n",
    "\n",
    "bronze_followers_table = \\\n",
    "  f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_FOLLOWERS_TABLE}\"\n",
    "\n",
    "\n",
    "# 3. execute the ingestion\n",
    "ingestion_timestamp = pd.Timestamp.utcnow()\n",
    "\n",
    "# extract the list of files from the pending folder\n",
    "historical_files_info = [\n",
    "    (f.path, pd.to_datetime(f.modificationTime, unit='ms', utc=True)) \n",
    "    for f in dbutils.fs.ls(landing_pending_folder)\n",
    "]\n",
    "\n",
    "for file_path, file_timestamp in historical_files_info:\n",
    "\n",
    "  # extract filename from file path\n",
    "  filename = file_path.split('/')[-1]\n",
    "  \n",
    "  # define source and target paths for file\n",
    "  pending_path = landing_pending_folder + filename\n",
    "  processed_path = landing_processed_folder + filename\n",
    "  errors_path = landing_errors_folder + filename\n",
    "\n",
    "  # check if filename is of expected format\n",
    "  if re.search(\n",
    "    r'Content_\\d{4}-\\d{2}-\\d{2}_\\d{4}-\\d{2}-\\d{2}_' \n",
    "    + LINKEDIN_PROFILE_NAME + r'\\.xlsx', filename\n",
    "  ):\n",
    "    # process valid filename\n",
    "    try:\n",
    "      # read totals from xlsx file\n",
    "      print(f\"Processing ENGAGEMENT sheet in {pending_path}\")\n",
    "      totals_df = pd.read_excel(\n",
    "        pending_path, \n",
    "        sheet_name=\"ENGAGEMENT\", \n",
    "        parse_dates=[\"Date\"]\n",
    "      ).dropna().iloc[:-1] \n",
    "      totals_df.columns = totals_df.columns.str.lower().str.replace(' ', '_') \n",
    "      totals_df['ingestion_timestamp'] = ingestion_timestamp\n",
    "      totals_df['source_file'] = filename\n",
    "      totals_df['source_file_timestamp'] = file_timestamp\n",
    "      \n",
    "      # Write totals to Delta table with upsert logic\n",
    "      print(f\"Writing to {bronze_totals_table}\")\n",
    "      if not totals_df.empty:\n",
    "        if spark.catalog.tableExists(bronze_totals_table):\n",
    "          delta_table = DeltaTable.forName(spark, bronze_totals_table)\n",
    "          delta_table.alias(\"t\").merge(\n",
    "            spark.createDataFrame(totals_df).alias(\"s\"),\n",
    "            \"t.date = s.date\"\n",
    "          ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "        else:\n",
    "          spark.createDataFrame(totals_df).write.format(\"delta\").saveAsTable(\n",
    "            bronze_totals_table\n",
    "          )\n",
    "\n",
    "      # read followers from xlsx file\n",
    "      print(f\"Processing FOLLOWERS sheet in {pending_path}\")\n",
    "      followers_df = pd.read_excel(\n",
    "        pending_path, \n",
    "        sheet_name=\"FOLLOWERS\", \n",
    "        parse_dates=[\"Date\"],\n",
    "        skiprows=2\n",
    "      ).dropna().iloc[:-1]\n",
    "      followers_df.columns = followers_df.columns.str.lower().str.replace(' ', '_') \n",
    "      followers_df['ingestion_timestamp'] = ingestion_timestamp\n",
    "      followers_df['source_file'] = filename\n",
    "      followers_df['source_file_timestamp'] = file_timestamp\n",
    "      \n",
    "      # Write followers to Delta table with upsert logic\n",
    "      print(f\"Writing to {bronze_followers_table}\")\n",
    "      if not followers_df.empty:\n",
    "        if spark.catalog.tableExists(bronze_followers_table):\n",
    "          delta_table = DeltaTable.forName(spark, bronze_followers_table)\n",
    "          delta_table.alias(\"t\").merge(\n",
    "            spark.createDataFrame(followers_df).alias(\"s\"),\n",
    "            \"t.date = s.date\"\n",
    "          ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "        else:\n",
    "          spark.createDataFrame(followers_df).write.format(\"delta\").saveAsTable(\n",
    "            bronze_followers_table\n",
    "          )\n",
    "\n",
    "      print(f\"Processed: Moving {pending_path} to {processed_path}\")\n",
    "      dbutils.fs.mv(pending_path, processed_path)\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      print(f\"Errors encountered: Moving {pending_path} to {errors_path}\")\n",
    "      dbutils.fs.mv(pending_path, errors_path)\n",
    "  else:\n",
    "    # move invalid filename to errors folder\n",
    "    try:\n",
    "      print(f\"Invalid filename: Moving {pending_path} to {errors_path}\")\n",
    "      dbutils.fs.mv(pending_path, errors_path)\n",
    "    except Exception as e:\n",
    "      print(f\"Failed to move file {pending_path}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6840877729571798,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze historical ingest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
