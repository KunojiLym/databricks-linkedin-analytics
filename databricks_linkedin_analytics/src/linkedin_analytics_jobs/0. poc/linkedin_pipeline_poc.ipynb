{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83d0e8f-90fe-465a-94a4-a08aa9bb420a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51386aa9-8140-4749-bd8d-d3c29cc4f1dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.help('ls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9e5a51a-adf3-4b46-a1db-10736e918b56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 1. Process historical analytics data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d452bcde-c52a-41a7-a2ae-14f2c2b107e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# 1. define our constants\n",
    "LINKEDIN_PROFILE_NAME = \"YingzhaoOuyang\" # use your own profile name here\n",
    "\n",
    "LANDING_CATALOG = \"landing\"\n",
    "LANDING_SCHEMA = \"linkedin\"\n",
    "LANDING_DAILY_VOLUME = \"content_historical\"\n",
    "\n",
    "PENDING_FOLDER = \"pending\"\n",
    "PROCESSED_FOLDER = \"processed\"\n",
    "ERRORS_FOLDER = \"errors\"\n",
    "\n",
    "BRONZE_CATALOG = \"bronze\"\n",
    "BRONZE_SCHEMA = \"linkedin\"\n",
    "BRONZE_TOTALS_TABLE = \"totals\"\n",
    "BRONZE_FOLLOWERS_TABLE = \"followers\"\n",
    "\n",
    "# 2. set our input and output variables\n",
    "source_volume = \\\n",
    "  f\"/Volumes/{LANDING_CATALOG}/{LANDING_SCHEMA}/{LANDING_DAILY_VOLUME}/\"\n",
    "\n",
    "landing_pending_folder = f\"{source_volume}{PENDING_FOLDER}/\"\n",
    "landing_processed_folder = f\"{source_volume}{PROCESSED_FOLDER}/\"\n",
    "landing_errors_folder = f\"{source_volume}{ERRORS_FOLDER}/\"\n",
    "\n",
    "bronze_totals_table = \\\n",
    "  f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_TOTALS_TABLE}\"\n",
    "\n",
    "bronze_followers_table = \\\n",
    "  f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_FOLLOWERS_TABLE}\"\n",
    "\n",
    "\n",
    "# 3. execute the ingestion\n",
    "ingestion_timestamp = pd.Timestamp.utcnow()\n",
    "\n",
    "# extract the list of files from the pending folder\n",
    "historical_files_info = [\n",
    "    (f.path, pd.to_datetime(f.modificationTime, unit='ms', utc=True)) \n",
    "    for f in dbutils.fs.ls(landing_pending_folder)\n",
    "]\n",
    "\n",
    "for file_path, file_timestamp in historical_files_info:\n",
    "\n",
    "  # extract filename from file path\n",
    "  filename = file_path.split('/')[-1]\n",
    "  \n",
    "  # define source and target paths for file\n",
    "  pending_path = landing_pending_folder + filename\n",
    "  processed_path = landing_processed_folder + filename\n",
    "  errors_path = landing_errors_folder + filename\n",
    "\n",
    "  # check if filename is of expected format\n",
    "  if re.search(\n",
    "    r'Content_\\d{4}-\\d{2}-\\d{2}_\\d{4}-\\d{2}-\\d{2}_' \n",
    "    + LINKEDIN_PROFILE_NAME + r'\\.xlsx', filename\n",
    "  ):\n",
    "    # process valid filename\n",
    "    try:\n",
    "      # read totals from xlsx file\n",
    "      print(f\"Processing ENGAGEMENT sheet in {pending_path}\")\n",
    "      totals_df = pd.read_excel(\n",
    "        pending_path, \n",
    "        sheet_name=\"ENGAGEMENT\", \n",
    "        parse_dates=[\"Date\"]\n",
    "      ).dropna().iloc[:-1] \n",
    "      totals_df.columns = totals_df.columns.str.lower().str.replace(' ', '_') \n",
    "      totals_df['ingestion_timestamp'] = ingestion_timestamp\n",
    "      totals_df['source_file'] = filename\n",
    "      totals_df['source_file_timestamp'] = file_timestamp\n",
    "      \n",
    "      # Write totals to Delta table with upsert logic\n",
    "      print(f\"Writing to {bronze_totals_table}\")\n",
    "      if not totals_df.empty:\n",
    "        if spark.catalog.tableExists(bronze_totals_table):\n",
    "          delta_table = DeltaTable.forName(spark, bronze_totals_table)\n",
    "          delta_table.alias(\"t\").merge(\n",
    "            spark.createDataFrame(totals_df).alias(\"s\"),\n",
    "            \"t.date = s.date\"\n",
    "          ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "        else:\n",
    "          spark.createDataFrame(totals_df).write.format(\"delta\").saveAsTable(\n",
    "            bronze_totals_table\n",
    "          )\n",
    "\n",
    "      # read followers from xlsx file\n",
    "      print(f\"Processing FOLLOWERS sheet in {pending_path}\")\n",
    "      followers_df = pd.read_excel(\n",
    "        pending_path, \n",
    "        sheet_name=\"FOLLOWERS\", \n",
    "        parse_dates=[\"Date\"],\n",
    "        skiprows=2\n",
    "      ).dropna().iloc[:-1]\n",
    "      followers_df.columns = followers_df.columns.str.lower().str.replace(' ', '_') \n",
    "      followers_df['ingestion_timestamp'] = ingestion_timestamp\n",
    "      followers_df['source_file'] = filename\n",
    "      followers_df['source_file_timestamp'] = file_timestamp\n",
    "      \n",
    "      # Write followers to Delta table with upsert logic\n",
    "      print(f\"Writing to {bronze_followers_table}\")\n",
    "      if not followers_df.empty:\n",
    "        if spark.catalog.tableExists(bronze_followers_table):\n",
    "          delta_table = DeltaTable.forName(spark, bronze_followers_table)\n",
    "          delta_table.alias(\"t\").merge(\n",
    "            spark.createDataFrame(followers_df).alias(\"s\"),\n",
    "            \"t.date = s.date\"\n",
    "          ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "        else:\n",
    "          spark.createDataFrame(followers_df).write.format(\"delta\").saveAsTable(\n",
    "            bronze_followers_table\n",
    "          )\n",
    "\n",
    "      print(f\"Processed: Moving {pending_path} to {processed_path}\")\n",
    "      dbutils.fs.mv(pending_path, processed_path)\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      print(f\"Errors encountered: Moving {pending_path} to {errors_path}\")\n",
    "      dbutils.fs.mv(pending_path, errors_path)\n",
    "  else:\n",
    "    # move invalid filename to errors folder\n",
    "    try:\n",
    "      print(f\"Invalid filename: Moving {pending_path} to {errors_path}\")\n",
    "      dbutils.fs.mv(pending_path, errors_path)\n",
    "    except Exception as e:\n",
    "      print(f\"Failed to move file {pending_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f8de9a-5588-4a47-ac94-af1de9c23560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Process daily analytics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22257c97-06d9-4924-b2bb-3160e74da7da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# 1. define our constants\n",
    "VERSION = 2\n",
    "\n",
    "LINKEDIN_PROFILE_NAME = \"YingzhaoOuyang\" # use your own profile name here\n",
    "\n",
    "LANDING_CATALOG = \"landing\"\n",
    "LANDING_SCHEMA = \"linkedin\"\n",
    "LANDING_DAILY_VOLUME = \"content_daily\"\n",
    "\n",
    "PENDING_FOLDER = \"pending\"\n",
    "PROCESSED_FOLDER = \"processed\"\n",
    "ERRORS_FOLDER = \"errors\"\n",
    "\n",
    "BRONZE_CATALOG = \"bronze\"\n",
    "BRONZE_SCHEMA = \"linkedin\"\n",
    "if VERSION > 1:\n",
    "  BRONZE_DISCOVERY_TABLE = \"discovery\"\n",
    "BRONZE_TOTALS_TABLE = \"totals\"\n",
    "BRONZE_FOLLOWERS_TABLE = \"followers\"\n",
    "\n",
    "BRONZE_IMPRESSIONS_TABLE = \"impressions\"\n",
    "BRONZE_ENGAGEMENTS_TABLE = \"engagements\"\n",
    "\n",
    "# 2. set our input and output variables\n",
    "source_volume = \\\n",
    "  f\"/Volumes/{LANDING_CATALOG}/{LANDING_SCHEMA}/{LANDING_DAILY_VOLUME}/\"\n",
    "\n",
    "landing_pending_folder = f\"{source_volume}{PENDING_FOLDER}/\"\n",
    "landing_processed_folder = f\"{source_volume}{PROCESSED_FOLDER}/\"\n",
    "landing_errors_folder = f\"{source_volume}{ERRORS_FOLDER}/\"\n",
    "\n",
    "bronze_totals_table = \\\n",
    "  f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_TOTALS_TABLE}\"\n",
    "if VERSION > 1:\n",
    "  bronze_discovery_table = \\\n",
    "    f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_DISCOVERY_TABLE}\"\n",
    "\n",
    "bronze_followers_table = \\\n",
    "  f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_FOLLOWERS_TABLE}\"\n",
    "\n",
    "bronze_impressions_table_prefix = \\\n",
    "    f'{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_IMPRESSIONS_TABLE}'\n",
    "bronze_engagements_table_prefix = \\\n",
    "    f'{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_ENGAGEMENTS_TABLE}'\n",
    "\n",
    "# 3. execute the ingestion\n",
    "ingestion_timestamp = pd.Timestamp.utcnow()\n",
    "\n",
    "# extract the list of files from the pending folder\n",
    "daily_files_info = [\n",
    "    (f.path, pd.to_datetime(f.modificationTime, unit='ms', utc=True)) \n",
    "    for f in dbutils.fs.ls(landing_pending_folder)\n",
    "]\n",
    "\n",
    "# get current count of tables in bronze schema\n",
    "table_count_in_bronze_schema = spark.sql(\n",
    "    f\"SHOW TABLES IN {BRONZE_CATALOG}.{BRONZE_SCHEMA}\"\n",
    ").count()\n",
    "dates_processed = []\n",
    "\n",
    "for file_path, file_timestamp in daily_files_info:\n",
    "\n",
    "  tables_to_create = 0  \n",
    "  \n",
    "  # extract filename from file path\n",
    "  filename = file_path.split('/')[-1]\n",
    "  \n",
    "  # define source and target paths for file\n",
    "  pending_path = landing_pending_folder + filename\n",
    "  processed_path = landing_processed_folder + filename\n",
    "  errors_path = landing_errors_folder + filename\n",
    "\n",
    "  # check if filename is of expected format (from and to date are the same)\n",
    "  if re.search(\n",
    "    r'Content_(\\d{4}-\\d{2}-\\d{2})_\\1_' \n",
    "    + LINKEDIN_PROFILE_NAME + r'\\.xlsx', filename\n",
    "  ):\n",
    "      \n",
    "    # extract date of analytics and append it to staging table names\n",
    "    analytics_date_str = re.search(\n",
    "      r'Content_(\\d{4}-\\d{2}-\\d{2})_\\1_', filename\n",
    "    ).group(1)\n",
    "    analytics_date = pd.to_datetime(analytics_date_str).date()\n",
    "    bronze_engagements_table = \\\n",
    "      f\"{bronze_engagements_table_prefix}_{\n",
    "          analytics_date_str.replace('-', '_')\n",
    "        }\"\n",
    "    bronze_impressions_table = \\\n",
    "      f\"{bronze_impressions_table_prefix}_{\n",
    "          analytics_date_str.replace('-', '_')\n",
    "        }\"\n",
    "\n",
    "    # check if bronze_engagement_table exists in bronze schema\n",
    "    if not spark.catalog.tableExists(bronze_engagements_table):\n",
    "      tables_to_create += 1\n",
    "\n",
    "    # check if bronze_impressions_table exists in bronze schema\n",
    "    if not spark.catalog.tableExists(bronze_impressions_table):\n",
    "      tables_to_create += 1\n",
    "\n",
    "    # exit if too many tables in bronze schema (specific to Databricks Free Edition)\n",
    "    if table_count_in_bronze_schema + tables_to_create > 100:\n",
    "        print(\"Too many tables in bronze schema. Please process and clear staging tables before rerunning.\")\n",
    "        break\n",
    "\n",
    "    # process valid filename\n",
    "    try:\n",
    "      if VERSION > 1:\n",
    "        try:\n",
    "          # read members reached from xlsx file\n",
    "          print(f\"Processing DISCOVERY sheet in {pending_path}\")\n",
    "          discovery_df = pd.read_excel(\n",
    "            pending_path, \n",
    "            sheet_name=\"DISCOVERY\", \n",
    "          ).set_index(\"Overall Performance\").transpose().dropna().reset_index(drop=True)\n",
    "          discovery_df.columns = discovery_df.columns.str.lower().str.replace(' ', '_') \n",
    "\n",
    "          discovery_df = discovery_df[['members_reached']]\n",
    "          discovery_df['date'] = analytics_date\n",
    "          discovery_df['ingestion_timestamp'] = ingestion_timestamp\n",
    "          discovery_df['source_file'] = filename\n",
    "          discovery_df['source_file_timestamp'] = file_timestamp\n",
    "\n",
    "          # Write members reached to Delta table with upsert logic\n",
    "          print(f\"Writing to {bronze_discovery_table}\")\n",
    "          if not discovery_df.empty:\n",
    "            if spark.catalog.tableExists(bronze_discovery_table):\n",
    "              delta_table = DeltaTable.forName(spark, bronze_discovery_table)\n",
    "              delta_table.alias(\"t\").merge(\n",
    "                spark.createDataFrame(discovery_df).alias(\"s\"),\n",
    "                \"t.date = s.date\"\n",
    "              ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "            else:\n",
    "              spark.createDataFrame(discovery_df).write.format(\"delta\").saveAsTable(\n",
    "                bronze_discovery_table\n",
    "              )\n",
    "        except Exception as e:\n",
    "          print(f\"Error writing to {bronze_discovery_table}: {e}\")\n",
    "          exit()\n",
    "\n",
    "      # read totals from xlsx file\n",
    "      print(f\"Processing ENGAGEMENT sheet in {pending_path}\")\n",
    "      totals_df = pd.read_excel(\n",
    "        pending_path, \n",
    "        sheet_name=\"ENGAGEMENT\", \n",
    "        parse_dates=[\"Date\"]\n",
    "      ).dropna()\n",
    "      totals_df.columns = totals_df.columns.str.lower().str.replace(' ', '_') \n",
    "      totals_df['ingestion_timestamp'] = ingestion_timestamp\n",
    "      totals_df['source_file'] = filename\n",
    "      totals_df['source_file_timestamp'] = file_timestamp\n",
    "      \n",
    "      # Write totals to Delta table with upsert logic\n",
    "      print(f\"Writing to {bronze_totals_table}\")\n",
    "      if not totals_df.empty:\n",
    "        if spark.catalog.tableExists(bronze_totals_table):\n",
    "          delta_table = DeltaTable.forName(spark, bronze_totals_table)\n",
    "          delta_table.alias(\"t\").merge(\n",
    "            spark.createDataFrame(totals_df).alias(\"s\"),\n",
    "            \"t.date = s.date\"\n",
    "          ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "        else:\n",
    "          spark.createDataFrame(totals_df).write.format(\"delta\").saveAsTable(\n",
    "            bronze_totals_table\n",
    "          )\n",
    "\n",
    "      # read followers from xlsx file\n",
    "      print(f\"Processing FOLLOWERS sheet in {pending_path}\")\n",
    "      followers_df = pd.read_excel(\n",
    "        pending_path, \n",
    "        sheet_name=\"FOLLOWERS\", \n",
    "        parse_dates=[\"Date\"],\n",
    "        skiprows=2\n",
    "      ).dropna()\n",
    "      followers_df.columns = followers_df.columns.str.lower().str.replace(' ', '_') \n",
    "      followers_df['ingestion_timestamp'] = ingestion_timestamp\n",
    "      followers_df['source_file'] = filename\n",
    "      followers_df['source_file_timestamp'] = file_timestamp\n",
    "      \n",
    "      # Write followers to Delta table with upsert logic\n",
    "      print(f\"Writing to {bronze_followers_table}\")\n",
    "      if not followers_df.empty:\n",
    "        if spark.catalog.tableExists(bronze_followers_table):\n",
    "          delta_table = DeltaTable.forName(spark, bronze_followers_table)\n",
    "          delta_table.alias(\"t\").merge(\n",
    "            spark.createDataFrame(followers_df).alias(\"s\"),\n",
    "            \"t.date = s.date\"\n",
    "          ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "        else:\n",
    "          spark.createDataFrame(followers_df).write.format(\"delta\").saveAsTable(\n",
    "            bronze_followers_table\n",
    "          )\n",
    "\n",
    "      # read top posts from xlsx file  \n",
    "      print(f\"Processing TOP POSTS sheet in {pending_path}\")\n",
    "      topposts_df = pd.read_excel(pending_path, sheet_name=\"TOP POSTS\", skiprows=2)\n",
    "\n",
    "      engagements_df = topposts_df.iloc[:, :3].dropna()\n",
    "      if not engagements_df.empty:  \n",
    "        engagements_df.columns = engagements_df.columns.str.replace(' ', '_').str.lower()\n",
    "        engagements_df['analytics_date'] = analytics_date\n",
    "        engagements_df['ingestion_timestamp'] = ingestion_timestamp\n",
    "        engagements_df['source_file'] = filename\n",
    "        engagements_df['source_file_timestamp'] = file_timestamp\n",
    "\n",
    "      impressions_df = topposts_df.iloc[:, 4:].dropna()\n",
    "      if not impressions_df.empty:\n",
    "        impressions_df.columns = impressions_df.columns.str.replace('.1', '', regex=False).str.replace(' ', '_').str.lower()\n",
    "        impressions_df['analytics_date'] = analytics_date\n",
    "        impressions_df['ingestion_timestamp'] = ingestion_timestamp\n",
    "        impressions_df['source_file'] = filename\n",
    "        impressions_df['source_file_timestamp'] = file_timestamp\n",
    "\n",
    "      # Write engagements to Delta staging table, overwrite existing      \n",
    "      if not engagements_df.empty:\n",
    "        print(f\"Writing to {bronze_engagements_table}\")\n",
    "        spark.createDataFrame(engagements_df).write.mode(\"overwrite\").saveAsTable(\n",
    "          bronze_engagements_table\n",
    "        )\n",
    "      else:\n",
    "        print(f\"Skipped writing empty dataset to {bronze_engagements_table}\")\n",
    "\n",
    "      # Write impressions to Delta staging table, overwrite existing\n",
    "      \n",
    "      if not impressions_df.empty:\n",
    "        print(f\"Writing to {bronze_impressions_table}\")\n",
    "        spark.createDataFrame(impressions_df).write.mode(\"overwrite\").saveAsTable(\n",
    "          bronze_impressions_table\n",
    "        )\n",
    "      else:\n",
    "        print(f\"Skipped writing empty dataset to {bronze_impressions_table}\")\n",
    "\n",
    "      # update counters for dates processed and tables in bronze schema\n",
    "      dates_processed.append(analytics_date_str)  \n",
    "      table_count_in_bronze_schema += tables_to_create\n",
    "\n",
    "      print(f\"Processed: Moving {pending_path} to {processed_path}\")\n",
    "      dbutils.fs.mv(pending_path, processed_path)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      print(f\"Errors encountered: Moving {pending_path} to {errors_path}\")\n",
    "      dbutils.fs.mv(pending_path, errors_path)\n",
    "  else:\n",
    "    # move invalid filename to errors folder\n",
    "    try:\n",
    "      print(f\"Invalid filename: Moving {pending_path} to {errors_path}\")\n",
    "      dbutils.fs.mv(pending_path, errors_path)\n",
    "    except Exception as e:\n",
    "      print(f\"Failed to move file {pending_path}: {e}\")\n",
    "\n",
    "if len(dates_processed) == 0:\n",
    "    print(\"No dates processed.\")\n",
    "else:\n",
    "    print(f\"Dates processed: {dates_processed}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594ce3e6-a66a-45f0-be6f-4c565624d780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# uncomment to reprocess files in errors folder\n",
    "\n",
    "# files = dbutils.fs.ls('/Volumes/landing/linkedin/content_daily/errors/')\n",
    "# for file in files:\n",
    "#     dbutils.fs.mv(file.path, '/Volumes/landing/linkedin/content_daily/pending/' + file.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de7e34a-ae64-4981-91be-74c31fdf9aa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# uncomment to reprocess files in processed folder\n",
    "\n",
    "# files = dbutils.fs.ls('/Volumes/landing/linkedin/content_daily/processed/')\n",
    "# for file in files:\n",
    "#     dbutils.fs.mv(file.path, '/Volumes/landing/linkedin/content_daily/pending/' + file.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "719ee866-ab18-4d4b-a9d4-5146ad7367b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# uncomment to drop staging tables\n",
    "\n",
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# staging_table_prefix = \"impressions_\"\n",
    "# tables_df = spark.catalog.listTables(\"bronze.linkedin\")\n",
    "# impressions_tables = [t.name for t in tables_df if t.name.startswith(staging_table_prefix)]\n",
    "# for table in impressions_tables:\n",
    "#     spark.sql(f\"DROP TABLE bronze.linkedin.{table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f2f4290-3dd1-410a-b1c6-a59671478962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Post table ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e440da-fcfb-448d-a950-5acaff52926e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions for fetching and parsing LinkedIn post HTML content\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def get_html_content(url: str, max_retries: int = 5, base_delay: float = 5.0):\n",
    "    \"\"\"\n",
    "    Fetches HTML content from a LinkedIn post URL with exponential backoff.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Fetching HTML content from URL: {url} (Attempt {attempt + 1})\")\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            print(f\"Successfully fetched HTML content from {url}\")\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                delay = base_delay * (2 ** attempt)\n",
    "                print(f\"Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(f\"Failed to fetch HTML content from {url} after {max_retries} attempts.\")\n",
    "                raise e\n",
    "\n",
    "def get_content(html_content):\n",
    "    \"\"\"\n",
    "    Extracts post content from HTML using BeautifulSoup.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    content_wrapper = soup.find('p', class_=\"attributed-text-segment-list__content\")\n",
    "    if content_wrapper:\n",
    "        return content_wrapper.get_text()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75235a8-4062-4f7e-ad06-1620a0e03ed6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"post_url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1764485893787}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"post_url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}},\"link\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1764486375856}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract posts data using post_url from all impressions tables and save to bronze posts table\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, row_number, desc\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "BRONZE_POSTS_TABLE = \"posts\"\n",
    "\n",
    "bronze_posts_table = f'{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_POSTS_TABLE}'\n",
    "\n",
    "# List all impressions staging tables in the bronze.linkedin schema\n",
    "daily_impressions_tables = [table.name for table in spark.catalog.listTables(f'{BRONZE_CATALOG}.{BRONZE_SCHEMA}') if table.name.startswith(BRONZE_IMPRESSIONS_TABLE)]\n",
    "\n",
    "print(\"Loading daily impressions tables to extract posts data...\")\n",
    "# Read each table and select the relevant columns\n",
    "columns_to_select = [\n",
    "  \"post_url\", \"post_publish_date\", \n",
    "  \"ingestion_timestamp\", \"source_file\", \"source_file_timestamp\"\n",
    "]\n",
    "dfs = [spark.read.table(f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{table}\").select(*columns_to_select) for table in daily_impressions_tables]\n",
    "\n",
    "# Union all dataframes and remove duplicates\n",
    "if dfs:\n",
    "    impressions_df = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        impressions_df = impressions_df.unionByName(df)\n",
    "    impressions_df = impressions_df.withColumn(\n",
    "        \"row_num\",\n",
    "        row_number().over(\n",
    "            Window.partitionBy(\"post_url\", \"post_publish_date\").orderBy(desc(\"ingestion_timestamp\"))\n",
    "        )\n",
    "    ).filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "    impressions_pd = impressions_df.toPandas()[columns_to_select]\n",
    "    impressions_pd['post_publish_date'] = pd.to_datetime(impressions_pd['post_publish_date'])\n",
    "else:\n",
    "    impressions_pd = pd.DataFrame(columns=columns_to_select)\n",
    "\n",
    "\n",
    "if not impressions_pd.empty:\n",
    "    # Display posts to be ingested\n",
    "    display(impressions_pd)\n",
    "\n",
    "       \n",
    "    # Fetch HTML content for each post URL\n",
    "    print(f\"Fetching HTML content for {len(impressions_pd)} posts...\")\n",
    "    impressions_pd['html_content'] = impressions_pd['post_url'].apply(get_html_content)\n",
    "    # Extract link, title, and content from the HTML content\n",
    "    impressions_pd['link'] = impressions_pd['html_content'].apply(lambda x: BeautifulSoup(x, \"html.parser\").find('link').get('href'))\n",
    "    impressions_pd['title'] = impressions_pd['html_content'].apply(lambda x: BeautifulSoup(x, \"html.parser\").find(\"head\").get_text(strip=True).split('|')[0].strip())\n",
    "    impressions_pd['content'] = impressions_pd['html_content'].apply(get_content)\n",
    "\n",
    "    impressions_spark_df = spark.createDataFrame(impressions_pd)\n",
    "    if not spark.catalog.tableExists(bronze_posts_table):\n",
    "        print(f\"Creating {bronze_posts_table} table...\")\n",
    "        impressions_spark_df.write.format(\"delta\").saveAsTable(bronze_posts_table)\n",
    "    else:\n",
    "        print(f\"Appending new posts to {bronze_posts_table} table...\")\n",
    "        impressions_spark_df.write.format(\"delta\").mode(\"append\").saveAsTable(bronze_posts_table)\n",
    "else:\n",
    "    print(\"No new posts found based on impressions tables.\")\n",
    "\n",
    "# Display sample of posts data for verification\n",
    "posts_sample_df = spark.read.table(bronze_posts_table).limit(10)\n",
    "display(posts_sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34411af5-7588-4eae-85b6-c3a408c1c929",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"post_url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}},\"link\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1764486020791}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "posts_df = spark.read.table(bronze_posts_table).collect()\n",
    "display(posts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "107f9a3c-4adf-4aa3-b997-23e92f6af616",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Ingest post analytics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac035331-28a6-44ed-87e4-08a9e320856d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_excel(\n",
    "        '/Volumes/landing/linkedin/posts/processed/PostAnalytics_YingzhaoOuyang_7367980676152819712.xlsx', \n",
    "        sheet_name=\"PERFORMANCE\", \n",
    "        header=None,\n",
    "        names=[\"key\", \"value\"],\n",
    "      ).dropna(how='all')\n",
    "\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05801260-cf38-4e24-949b-e0e8234cae05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not test_df[test_df['key'].str.contains(\" Highlights \", na=False)]['key'].empty:\n",
    "    print(\"Found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f88a46f-5c3b-4163-a85c-5635748e75ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not test_df[test_df['key'].str.contains(\" Highlights \", na=False)]['key'].empty:\n",
    "\n",
    "    date_range_str = test_df[test_df['key'].str.contains(\" Highlights \", na=False)]['key'].str.split(\" Highlights \").iloc[0][1]\n",
    "    [date_start_str, date_end_str] = date_range_str.split(\" to \")\n",
    "\n",
    "    date_start = pd.to_datetime(date_start_str).date()\n",
    "    date_end = pd.to_datetime(date_end_str).date()\n",
    "\n",
    "else:\n",
    "    print(\"Cannot detect analytics date\")\n",
    "\n",
    "date_start, date_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c94102-50a5-468d-b47c-9d4f6e4c0b95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timedelta_str = test_df.dropna().set_index('key').transpose()[\"Post Publish Time\"].iloc[0]\n",
    "\n",
    "pd.to_datetime(date_start_str + ' ' + timedelta_str + ' UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c721b27-ab54-4ab5-9917-f61335739ab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "post_url = test_df.dropna().set_index('key').transpose()[\"Post URL\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de7e96f-d3d8-4719-9ca2-3c2c0c199c59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "html_content = get_html_content(post_url)\n",
    "link = BeautifulSoup(html_content, \"html.parser\").find('link').get('href')\n",
    "\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feba5251-a544-43ba-8286-54caeac9d1fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_df.dropna().set_index('key').transpose().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cc37617-4194-493b-b9cf-9c7826fb3c38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# 1. define our constants\n",
    "LINKEDIN_PROFILE_NAME = \"YingzhaoOuyang\" # use your own profile name here\n",
    "\n",
    "LANDING_CATALOG = \"landing\"\n",
    "LANDING_SCHEMA = \"linkedin\"\n",
    "LANDING_POSTS_VOLUME = \"posts\"\n",
    "\n",
    "PENDING_FOLDER = \"pending\"\n",
    "PROCESSED_FOLDER = \"processed\"\n",
    "ERRORS_FOLDER = \"errors\"\n",
    "\n",
    "BRONZE_CATALOG = \"bronze\"\n",
    "BRONZE_SCHEMA = \"linkedin\"\n",
    "BRONZE_POST_DETAILS_TABLE = \"post_details\"\n",
    "\n",
    "# 2. set our input and output variables\n",
    "source_volume = \\\n",
    "  f\"/Volumes/{LANDING_CATALOG}/{LANDING_SCHEMA}/{LANDING_POSTS_VOLUME}/\"\n",
    "\n",
    "landing_pending_folder = f\"{source_volume}{PENDING_FOLDER}/\"\n",
    "landing_processed_folder = f\"{source_volume}{PROCESSED_FOLDER}/\"\n",
    "landing_errors_folder = f\"{source_volume}{ERRORS_FOLDER}/\"\n",
    "\n",
    "bronze_post_details_table = \\\n",
    "  f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_POST_DETAILS_TABLE}\"\n",
    "\n",
    "# 3. execute the ingestion\n",
    "ingestion_timestamp = datetime.datetime.utcnow()\n",
    "\n",
    "# extract the list of files from the pending folder\n",
    "post_files_info = [\n",
    "    (f.path, pd.to_datetime(f.modificationTime, unit='ms', utc=True).to_pydatetime()) \n",
    "    for f in dbutils.fs.ls(landing_pending_folder)\n",
    "]\n",
    "\n",
    "for file_path, file_timestamp in post_files_info:\n",
    " \n",
    "  # extract filename from file path\n",
    "  filename = file_path.split('/')[-1]\n",
    "  \n",
    "  # define source and target paths for file\n",
    "  pending_path = landing_pending_folder + filename\n",
    "  processed_path = landing_processed_folder + filename\n",
    "  errors_path = landing_errors_folder + filename\n",
    "\n",
    "  # check if filename is of expected format\n",
    "  if re.search(\n",
    "    r'PostAnalytics_' + LINKEDIN_PROFILE_NAME + r'_\\d+(?: \\(\\d+\\))?\\.xlsx', filename\n",
    "  ):\n",
    "      \n",
    "    # process valid filename\n",
    "    try:\n",
    "      # read totals from xlsx file\n",
    "      print(f\"Extracting post URL and timestamp from PERFORMANCE sheet in {pending_path}\")\n",
    "      post_details_raw_df = pd.read_excel(\n",
    "        pending_path, \n",
    "        sheet_name=\"PERFORMANCE\", \n",
    "        header=None,\n",
    "        names=[\"key\", \"value\"],\n",
    "      ).dropna(how='all')\n",
    "\n",
    "      # Extract timestamp. post URL and date of analytics\n",
    "      transposed_post_details_df = post_details_raw_df.dropna().set_index('key').transpose()\n",
    "      post_timedelta_str = transposed_post_details_df[\"Post Publish Time\"].iloc[0]\n",
    "\n",
    "      # Extract date range of analytics from specific string pattern\n",
    "      df_with_date_range = post_details_raw_df[post_details_raw_df['key'].str.contains(\n",
    "          \" Highlights \", na=False\n",
    "        )]['key']\n",
    "      if df_with_date_range.empty:\n",
    "        print(\"Cannot detect analytics date\")\n",
    "        date_start_str = transposed_post_details_df[\"Post Date\"].iloc[0]\n",
    "        date_end_str = None\n",
    "      else:\n",
    "        date_range_str = \\\n",
    "          df_with_date_range.str.split(\" Highlights \").iloc[0][1]\n",
    "        [date_start_str, date_end_str] = date_range_str.split(\" to \")\n",
    "\n",
    "      post_timestamp = pd.to_datetime(date_start_str + ' ' + timedelta_str + ' UTC').to_pydatetime()\n",
    "\n",
    "      post_url = transposed_post_details_df[\"Post URL\"].iloc[0]\n",
    "      if date_end_str:\n",
    "        analytics_date = pd.to_datetime(date_end_str).to_pydatetime().date()\n",
    "      else:\n",
    "        analytics_date = None\n",
    "\n",
    "      post_top_demographics_df = pd.read_excel(\n",
    "        pending_path, \n",
    "        sheet_name=\"TOP DEMOGRAPHICS\", \n",
    "      ).dropna()\n",
    "      post_top_demographics_df.columns = post_top_demographics_df.columns.str.lower().str.replace('%', 'percent')\n",
    "\n",
    "      # Fetch HTML content for each post URL\n",
    "      print(f\"Fetching HTML content and link for {post_url}\")\n",
    "      html_content = get_html_content(post_url)\n",
    "      # Extract link from the HTML content\n",
    "      link = BeautifulSoup(html_content, \"html.parser\").find('link').get('href')\n",
    "      \n",
    "      print(f\"Preparing post details record for {link}\")\n",
    "      post_details_record = Row(\n",
    "          post_url=post_url, \n",
    "          link=link,\n",
    "          post_timestamp=post_timestamp,\n",
    "          analytics_date=analytics_date,\n",
    "          performace_metrics=str(transposed_post_details_df.to_dict()),\n",
    "          demographics_metrics=str(post_top_demographics_df.to_dict()),\n",
    "          ingestion_timestamp=ingestion_timestamp,\n",
    "          source_file=file_path,\n",
    "          source_file_timestamp=file_timestamp,\n",
    "      )\n",
    "\n",
    "      # Write post_details_record to Delta table with upsert logic based on latest analytics date\n",
    "      print(f\"Writing to {bronze_post_details_table}: {post_details_record}\")\n",
    "      post_details_spark_df = spark.createDataFrame([post_details_record])\n",
    "      if spark.catalog.tableExists(bronze_post_details_table):\n",
    "          delta_table = DeltaTable.forName(spark, bronze_post_details_table)\n",
    "          delta_table.alias(\"t\").merge(\n",
    "              post_details_spark_df.alias(\"s\"),\n",
    "              \"t.post_url = s.post_url and t.analytics_date = s.analytics_date\"\n",
    "          ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "      else:\n",
    "          post_details_spark_df.write.format(\"delta\").saveAsTable(\n",
    "            bronze_post_details_table\n",
    "          )\n",
    "\n",
    "      print(f\"Processed: Moving {pending_path} to {processed_path}\")\n",
    "      dbutils.fs.mv(pending_path, processed_path)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      print(f\"Errors encountered: Moving {pending_path} to {errors_path}\")\n",
    "      dbutils.fs.mv(pending_path, errors_path)\n",
    "  else:\n",
    "    # move invalid filename to errors folder\n",
    "    try:\n",
    "      print(f\"Invalid filename: Moving {pending_path} to {errors_path}\")\n",
    "      dbutils.fs.mv(pending_path, errors_path)\n",
    "    except Exception as e:\n",
    "      print(f\"Failed to move file {pending_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1028bd0b-93ef-4efa-ab0e-d883b1172f06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# uncomment to reprocess files in errors folder\n",
    "\n",
    "# files = dbutils.fs.ls('/Volumes/landing/linkedin/posts/errors/')\n",
    "# for file in files:\n",
    "#     dbutils.fs.mv(file.path, '/Volumes/landing/linkedin/posts/pending/' + file.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "419bf894-f562-4702-ba83-9e7afa6d2633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5. Silver Layer: Impressions ETL\n",
    "\n",
    "This section processes daily bronze impressions tables, standardizes date formats, merges data into the silver impressions table, and logs ingestion events. Source tables are dropped after processing to maintain workspace hygiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "642c4674-160b-49f2-b9d7-2944a03dddf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import sum\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# 1. define our constants\n",
    "LINKEDIN_PROFILE_NAME = \"YingzhaoOuyang\" # use your own profile name here\n",
    "\n",
    "BRONZE_CATALOG = \"bronze\"\n",
    "BRONZE_SCHEMA = \"linkedin\"\n",
    "BRONZE_IMPRESSIONS_TABLE_PREFIX = \"impressions\"\n",
    "BRONZE_TOTALS_TABLE = \"totals\"\n",
    "\n",
    "SILVER_CATALOG = \"silver\"\n",
    "SILVER_SCHEMA = \"linkedin\"\n",
    "SILVER_IMPRESSIONS_TABLE = \"impressions\"\n",
    "\n",
    "# 2. set our input and output variables\n",
    "\n",
    "# extract list of daily bronze staging impressions tables\n",
    "bronze_impressions_tables = [\n",
    "    table.name for table in spark.catalog.listTables(f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}\")\n",
    "    if re.match(rf\"{BRONZE_IMPRESSIONS_TABLE_PREFIX}_\\d{{4}}_\\d{{2}}_\\d{{2}}$\", table.name)\n",
    "]\n",
    "\n",
    "silver_impressions_table = \\\n",
    "  f\"{SILVER_CATALOG}.{SILVER_SCHEMA}.{SILVER_IMPRESSIONS_TABLE}\"\n",
    "\n",
    "# 3. execute the ingestion\n",
    "ingestion_timestamp = datetime.datetime.utcnow()\n",
    "\n",
    "# Process daily bronze staging impressions tables and merge into silver layer\n",
    "\n",
    "print(\"Processing daily impressions tables...\")\n",
    "\n",
    "for bronze_impression_table in bronze_impressions_tables:\n",
    "    # Extract date suffix from table name and convert to datetime in %Y_%m_%d format\n",
    "    date_str = bronze_impression_table.replace(f\"{BRONZE_IMPRESSIONS_TABLE_PREFIX}_\", \"\")\n",
    "    analytics_date = pd.to_datetime(date_str, format='%Y_%m_%d').date()\n",
    "\n",
    "    table_name = f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{bronze_impression_table}\"\n",
    "    \n",
    "    print(f\"Processing {table_name}...\")\n",
    "\n",
    "    silver_impressions_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        post_url,\n",
    "        to_date(post_publish_date, 'M/d/yyyy') AS post_publish_date,\n",
    "        impressions,\n",
    "        to_date('{date_str}', 'yyyy_MM_dd') AS analytics_date \n",
    "    FROM {table_name}\n",
    "    \"\"\")\n",
    "\n",
    "    silver_impressions_totals_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        impressions,\n",
    "        source_file,\n",
    "        source_file_timestamp,\n",
    "        ingestion_timestamp\n",
    "    FROM {BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_TOTALS_TABLE}\n",
    "    WHERE to_date('{date_str}', 'yyyy_MM_dd') = date\n",
    "    \"\"\")\n",
    "\n",
    "    impressions_others = silver_impressions_totals_df.select(\"impressions\").collect()[0].impressions - silver_impressions_df.agg(sum(\"impressions\")).collect()[0][0]\n",
    "    if impressions_others > 0:\n",
    "        silver_impressions_df = silver_impressions_df.union(\n",
    "            spark.createDataFrame([\n",
    "                Row(\n",
    "                    post_url = \"others\",\n",
    "                    post_publish_date = analytics_date,\n",
    "                    impressions = impressions_others,\n",
    "                    analytics_date = analytics_date,\n",
    "                    # ingestion_timestamp = silver_impressions_totals_df.select(\"ingestion_timestamp\").collect()[0].ingestion_timestamp,\n",
    "                    # source_file = silver_impressions_totals_df.select(\"source_file\").collect()[0].source_file,\n",
    "                    # source_file_timestamp = silver_impressions_totals_df.select(\"source_file_timestamp\").collect()[0].source_file_timestamp\n",
    "                )\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    if spark.catalog.tableExists(silver_impressions_table):\n",
    "        print(f\"Table {silver_impressions_table} exists, merging data...\")\n",
    "        delta_table = DeltaTable.forName(spark, silver_impressions_table)\n",
    "        (\n",
    "            delta_table.alias(\"t\")\n",
    "            .merge(\n",
    "                silver_impressions_df.alias(\"s\"),\n",
    "                \"t.post_url = s.post_url AND t.analytics_date = s.analytics_date\"\n",
    "            )\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Table {silver_impressions_table} does not exist, creating...\")\n",
    "        silver_impressions_df.write.format(\"delta\").saveAsTable(silver_impressions_table)\n",
    "    \n",
    "    # Drop the source table after processing\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    print(f\"Dropped table {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f934943-fdfe-4d70-9533-ce9c6246b4ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 6. Silver Layer: Engagements ETL\n",
    "This section processes daily bronze engagements tables, standardizes date formats, merges data into the silver engagemetns table, and logs ingestion events. Source tables are dropped after processing to maintain workspace hygiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b0b3e1-f407-487c-896a-144e49812cae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import sum\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# 1. define our constants\n",
    "LINKEDIN_PROFILE_NAME = \"YingzhaoOuyang\" # use your own profile name here\n",
    "\n",
    "BRONZE_CATALOG = \"bronze\"\n",
    "BRONZE_SCHEMA = \"linkedin\"\n",
    "BRONZE_ENGAGEMENTS_TABLE_PREFIX = \"engagements\"\n",
    "BRONZE_TOTALS_TABLE = \"totals\"\n",
    "\n",
    "SILVER_CATALOG = \"silver\"\n",
    "SILVER_SCHEMA = \"linkedin\"\n",
    "SILVER_ENGAGEMENTS_TABLE = \"engagements\"\n",
    "\n",
    "# 2. set our input and output variables\n",
    "\n",
    "# extract list of daily bronze staging engagements tables\n",
    "bronze_engagements_tables = [\n",
    "    table.name for table in spark.catalog.listTables(f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}\")\n",
    "    if re.match(rf\"{BRONZE_ENGAGEMENTS_TABLE_PREFIX}_\\d{{4}}_\\d{{2}}_\\d{{2}}$\", table.name)\n",
    "]\n",
    "\n",
    "silver_engagements_table = \\\n",
    "  f\"{SILVER_CATALOG}.{SILVER_SCHEMA}.{SILVER_ENGAGEMENTS_TABLE}\"\n",
    "\n",
    "# 3. execute the ingestion\n",
    "ingestion_timestamp = datetime.datetime.utcnow()\n",
    "\n",
    "# Process daily bronze staging engagements tables and merge into silver layer\n",
    "\n",
    "print(\"Processing daily engagements tables...\")\n",
    "\n",
    "for bronze_engagements_table in bronze_engagements_tables:\n",
    "    # Extract date suffix from table name and convert to datetime in %Y_%m_%d format\n",
    "    date_str = bronze_engagements_table.replace(f\"{BRONZE_ENGAGEMENTS_TABLE_PREFIX}_\", \"\")\n",
    "    analytics_date = pd.to_datetime(date_str, format='%Y_%m_%d').date()\n",
    "\n",
    "    table_name = f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{bronze_engagements_table}\"\n",
    "    \n",
    "    print(f\"Processing {table_name}...\")\n",
    "\n",
    "    silver_engagements_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        post_url,\n",
    "        to_date(post_publish_date, 'M/d/yyyy') AS post_publish_date,\n",
    "        engagements,\n",
    "        to_date('{date_str}', 'yyyy_MM_dd') AS analytics_date \n",
    "    FROM {table_name}\n",
    "    \"\"\")\n",
    "\n",
    "    silver_engagements_totals_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        engagements,\n",
    "        source_file,\n",
    "        source_file_timestamp,\n",
    "        ingestion_timestamp\n",
    "    FROM {BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_TOTALS_TABLE}\n",
    "    WHERE to_date('{date_str}', 'yyyy_MM_dd') = date\n",
    "    \"\"\")\n",
    "\n",
    "    engagements_others = silver_engagements_totals_df.select(\"engagements\").collect()[0].engagements - silver_engagements_df.agg(sum(\"engagements\")).collect()[0][0]\n",
    "    if engagements_others > 0:\n",
    "        silver_engagements_df = silver_engagements_df.union(\n",
    "            spark.createDataFrame([\n",
    "                Row(\n",
    "                    post_url = \"others\",\n",
    "                    post_publish_date = analytics_date,\n",
    "                    engagements = engagements_others,\n",
    "                    analytics_date = analytics_date,\n",
    "                    # ingestion_timestamp = silver_engagements_totals_df.select(\"ingestion_timestamp\").collect()[0].ingestion_timestamp,\n",
    "                    # source_file = silver_engagements_totals_df.select(\"source_file\").collect()[0].source_file,\n",
    "                    # source_file_timestamp = silver_engagements_totals_df.select(\"source_file_timestamp\").collect()[0].source_file_timestamp\n",
    "                )\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    if spark.catalog.tableExists(silver_engagements_table):\n",
    "        print(f\"Table {silver_engagements_table} exists, merging data...\")\n",
    "        delta_table = DeltaTable.forName(spark, silver_engagements_table)\n",
    "        (\n",
    "            delta_table.alias(\"t\")\n",
    "            .merge(\n",
    "                silver_engagements_df.alias(\"s\"),\n",
    "                \"t.post_url = s.post_url AND t.analytics_date = s.analytics_date\"\n",
    "            )\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Table {silver_engagements_table} does not exist, creating...\")\n",
    "        silver_engagements_df.write.format(\"delta\").saveAsTable(silver_engagements_table)\n",
    "    \n",
    "    # Drop the source table after processing\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    print(f\"Dropped table {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5123ec9d-bebd-48ed-b185-7f1a41d37329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#7. Silver Layer: Fill in-between dates for engagement and impressions tables with 0 values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a14304ad-9d23-425a-9478-31e6672f43b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, sequence, explode, to_date, min as spark_min, max as spark_max\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import sum\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# 1. define our constants\n",
    "LINKEDIN_PROFILE_NAME = \"YingzhaoOuyang\" # use your own profile name here\n",
    "\n",
    "SILVER_CATALOG = \"silver\"\n",
    "SILVER_SCHEMA = \"linkedin\"\n",
    "SILVER_IMPRESSIONS_TABLE = \"impressions\"\n",
    "SILVER_ENGAGEMENTS_TABLE = \"engagements\"\n",
    "\n",
    "silver_impressions_table = \\\n",
    "  f\"{SILVER_CATALOG}.{SILVER_SCHEMA}.{SILVER_IMPRESSIONS_TABLE}\"\n",
    "silver_engagements_table = \\\n",
    "  f\"{SILVER_CATALOG}.{SILVER_SCHEMA}.{SILVER_ENGAGEMENTS_TABLE}\"\n",
    "\n",
    "\n",
    "def fill_missing_dates(df, date_col, group_cols, value_col):\n",
    "    # Get min and max dates\n",
    "    date_range = df.select(spark_min(date_col).alias(\"min_date\"), spark_max(date_col).alias(\"max_date\")).collect()[0]\n",
    "    min_date, max_date = date_range.min_date, date_range.max_date\n",
    "\n",
    "    # Create full date sequence\n",
    "    date_seq_df = spark.createDataFrame([(min_date, max_date)], [\"start\", \"end\"]) \\\n",
    "        .select(explode(sequence(col(\"start\"), col(\"end\"))).alias(date_col))\n",
    "\n",
    "    # Cross join with unique post_url and post_publish_date\n",
    "    unique_keys_df = df.select(*group_cols).distinct()\n",
    "    full_grid_df = unique_keys_df.crossJoin(date_seq_df)\n",
    "\n",
    "    # Left join to original df\n",
    "    filled_df = full_grid_df.join(\n",
    "        df,\n",
    "        on=group_cols + [date_col],\n",
    "        how=\"left\"\n",
    "    ).fillna({value_col: 0})\n",
    "\n",
    "    return filled_df\n",
    "\n",
    "# Fill missing dates for silver impressions\n",
    "silver_impressions_filled_df = fill_missing_dates(\n",
    "    spark.table(silver_impressions_table),\n",
    "    \"analytics_date\",\n",
    "    [\"post_url\", \"post_publish_date\"],\n",
    "    \"impressions\"\n",
    ")\n",
    "\n",
    "# Fill missing dates for silver engagements\n",
    "silver_engagements_filled_df = fill_missing_dates(\n",
    "    spark.table(silver_engagements_table),\n",
    "    \"analytics_date\",\n",
    "    [\"post_url\", \"post_publish_date\"],\n",
    "    \"engagements\"\n",
    ")\n",
    "\n",
    "display(silver_impressions_filled_df)\n",
    "display(silver_engagements_filled_df)\n",
    "\n",
    "# silver_impressions_filled_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(silver_impressions_table)\n",
    "# silver_engagements_filled_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(silver_engagements_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "756bec99-58ec-4dbb-b21e-6820eb411627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9805097d-d6dc-4314-9932-a43e4469903c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 8. Silver Layer: Create totals and followers views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb33a1b-e972-4e99-9276-55fd5688c8af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VIEW IF NOT EXISTS silver.linkedin.totals AS SELECT date, impressions, engagements FROM bronze.linkedin.totals;\n",
    "CREATE VIEW IF NOT EXISTS silver.linkedin.followers AS SELECT date, new_followers FROM bronze.linkedin.followers;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7aa7f2b-eb6a-4cb9-ba78-1e38fc520dcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 9. Silver Layer: Posts ETL\n",
    "This section merges bronze posts and patch data, enriches post metadata, and writes the results to the silver posts table. Ingestion events are logged for traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c60b0961-d023-4e33-bbf1-ac3c8927a8f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 1. Join patch to posts on post_url\n",
    "-- Prioritize patch data over posts data\n",
    "-- Deduplicate merged_posts to ensure one row per post_url\n",
    "CREATE OR REPLACE TEMP VIEW merged_posts_dedup AS\n",
    "SELECT\n",
    "  post_url,\n",
    "  post_publish_date,\n",
    "  post_publish_timestamp,\n",
    "  link,\n",
    "  title,\n",
    "  content\n",
    "FROM (\n",
    "  SELECT\n",
    "    posts.post_url AS post_url,\n",
    "    CAST(posts.post_publish_date AS DATE) AS post_publish_date,\n",
    "    post_details.post_timestamp AS post_publish_timestamp,\n",
    "    COALESCE(patch.true_url, posts.link) AS link,\n",
    "    COALESCE(patch.title, posts.title) AS title,\n",
    "    COALESCE(patch.content, posts.content, posts.title) AS content,\n",
    "    ROW_NUMBER() OVER (\n",
    "      PARTITION BY posts.post_url \n",
    "      ORDER BY \n",
    "        patch.true_url DESC, \n",
    "        patch.title DESC, \n",
    "        patch.content DESC\n",
    "    ) AS rn\n",
    "  FROM\n",
    "    bronze.linkedin.posts AS posts\n",
    "  LEFT JOIN\n",
    "    bronze.linkedin.linkedin_patch AS patch\n",
    "  ON\n",
    "    posts.post_url = patch.post_url\n",
    "  LEFT JOIN\n",
    "    bronze.linkedin.post_details AS post_details\n",
    "  ON\n",
    "    posts.post_url = post_details.post_url\n",
    ")\n",
    "WHERE rn = 1;\n",
    "\n",
    "-- 2. Create silver posts table if not exists\n",
    "CREATE TABLE IF NOT EXISTS silver.linkedin.posts (\n",
    "  post_url STRING,\n",
    "  post_publish_date DATE,\n",
    "  post_publish_timestamp TIMESTAMP,\n",
    "  link STRING,\n",
    "  title STRING,\n",
    "  content STRING\n",
    ") USING DELTA;\n",
    "\n",
    "-- 3. Use deduplicated view for MERGE\n",
    "MERGE INTO silver.linkedin.posts AS t\n",
    "USING merged_posts_dedup AS s\n",
    "ON t.post_url = s.post_url\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    post_url = s.post_url,\n",
    "    post_publish_date = s.post_publish_date,\n",
    "    post_publish_timestamp = s.post_publish_timestamp,\n",
    "    link = s.link,\n",
    "    title = s.title,\n",
    "    content = s.content\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (\n",
    "    post_url, \n",
    "    post_publish_date, \n",
    "    post_publish_timestamp, \n",
    "    link, \n",
    "    title, \n",
    "    content\n",
    "  )\n",
    "  VALUES (\n",
    "    s.post_url, \n",
    "    s.post_publish_date, \n",
    "    s.post_publish_timestamp, \n",
    "    s.link, \n",
    "    s.title, \n",
    "    s.content\n",
    "  );"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6840877729571798,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "linkedin_pipeline_poc",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
